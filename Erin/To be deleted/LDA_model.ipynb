{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a78986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fd77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c638d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5b61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e33a5ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>['good', 'helpfull', 'read', 'book', 'good', '...</td>\n",
       "      <td>good helpfull read book good type thats find d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>['sadly', 'overprice', 'irrelevant', 'spite', ...</td>\n",
       "      <td>sadly overprice irrelevant spite claim illustr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>['endless', 'rant', 'howard', 'borrow', 'denni...</td>\n",
       "      <td>endless rant howard borrow dennis miller start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>['not', 'quite', 'hip', 'really', 'shame', 'ti...</td>\n",
       "      <td>not quite hip really shame time reserch go thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>['journey', 'centre', 'earth', 'hey', 'great',...</td>\n",
       "      <td>journey centre earth hey great book absolutely...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text  \\\n",
       "0    3.0  ['good', 'helpfull', 'read', 'book', 'good', '...   \n",
       "1    1.0  ['sadly', 'overprice', 'irrelevant', 'spite', ...   \n",
       "2    2.0  ['endless', 'rant', 'howard', 'borrow', 'denni...   \n",
       "3    1.0  ['not', 'quite', 'hip', 'really', 'shame', 'ti...   \n",
       "4    5.0  ['journey', 'centre', 'earth', 'hey', 'great',...   \n",
       "\n",
       "                                             combine  \n",
       "0  good helpfull read book good type thats find d...  \n",
       "1  sadly overprice irrelevant spite claim illustr...  \n",
       "2  endless rant howard borrow dennis miller start...  \n",
       "3  not quite hip really shame time reserch go thi...  \n",
       "4  journey centre earth hey great book absolutely...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://drive.google.com/file/d/1XY8kQd7kEivHHmU3jdZAZD8s0Anb8XsP/view'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "trainingDf = pd.read_csv(url)\n",
    "trainingDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b96983ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dc69f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = trainingDf[\"combine\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d7ca8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [[text for text in doc.split()] for doc in doc]\n",
    "dict_LoS = corpora.Dictionary(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8eaee9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(167056 unique tokens: ['book', 'build', 'buy', 'deep', 'detail']...)\n"
     ]
    }
   ],
   "source": [
    "print(dict_LoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "225be34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating corpus and saving to pickle\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating corpus and saving to pickle\")\n",
    "bow_corpus = [dict_LoS.doc2bow(w.split()) for w in processed_docs]\n",
    "pickle.dump(bow_corpus, open('bow_corpusE.pkl', 'wb'))\n",
    "pickle.dump(dict_LoS, open('dictionaryE.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7de0ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 17 (\"really\") appears 1 time.\n",
      "Word 41 (\"completely\") appears 1 time.\n",
      "Word 69 (\"half\") appears 2 time.\n",
      "Word 90 (\"none\") appears 1 time.\n",
      "Word 91 (\"not\") appears 2 time.\n",
      "Word 95 (\"one\") appears 1 time.\n",
      "Word 126 (\"time\") appears 1 time.\n",
      "Word 171 (\"solution\") appears 1 time.\n",
      "Word 199 (\"even\") appears 1 time.\n",
      "Word 216 (\"like\") appears 1 time.\n",
      "Word 219 (\"must\") appears 1 time.\n",
      "Word 272 (\"end\") appears 1 time.\n",
      "Word 311 (\"first\") appears 1 time.\n",
      "Word 339 (\"different\") appears 1 time.\n",
      "Word 357 (\"novel\") appears 1 time.\n",
      "Word 367 (\"plot\") appears 2 time.\n",
      "Word 369 (\"problem\") appears 1 time.\n",
      "Word 424 (\"run\") appears 1 time.\n",
      "Word 430 (\"something\") appears 2 time.\n",
      "Word 486 (\"enjoy\") appears 1 time.\n",
      "Word 488 (\"especially\") appears 1 time.\n",
      "Word 495 (\"fiction\") appears 1 time.\n",
      "Word 501 (\"himself\") appears 1 time.\n",
      "Word 594 (\"move\") appears 2 time.\n",
      "Word 598 (\"part\") appears 1 time.\n",
      "Word 633 (\"formulaic\") appears 1 time.\n",
      "Word 640 (\"least\") appears 1 time.\n",
      "Word 651 (\"stuff\") appears 1 time.\n",
      "Word 834 (\"difference\") appears 1 time.\n",
      "Word 840 (\"editor\") appears 1 time.\n",
      "Word 872 (\"mystery\") appears 1 time.\n",
      "Word 1017 (\"crime\") appears 1 time.\n",
      "Word 1081 (\"religion\") appears 1 time.\n",
      "Word 1085 (\"second\") appears 1 time.\n",
      "Word 1256 (\"everyone\") appears 2 time.\n",
      "Word 1294 (\"include\") appears 1 time.\n",
      "Word 1389 (\"connect\") appears 2 time.\n",
      "Word 1488 (\"brings\") appears 1 time.\n",
      "Word 1685 (\"general\") appears 1 time.\n",
      "Word 1708 (\"pretty\") appears 1 time.\n",
      "Word 1760 (\"relevant\") appears 1 time.\n",
      "Word 2251 (\"spend\") appears 1 time.\n",
      "Word 2950 (\"comparison\") appears 1 time.\n",
      "Word 3123 (\"concrete\") appears 1 time.\n",
      "Word 3491 (\"proverbial\") appears 1 time.\n",
      "Word 4551 (\"river\") appears 1 time.\n",
      "Word 4871 (\"willing\") appears 1 time.\n",
      "Word 4938 (\"truck\") appears 1 time.\n",
      "Word 5855 (\"apt\") appears 1 time.\n",
      "Word 6011 (\"liked\") appears 1 time.\n",
      "Word 7469 (\"whereas\") appears 1 time.\n",
      "Word 8947 (\"exhaust\") appears 1 time.\n",
      "Word 9736 (\"carl\") appears 1 time.\n",
      "Word 10834 (\"shannon\") appears 4 time.\n",
      "Word 13271 (\"chandler\") appears 1 time.\n",
      "Word 13272 (\"cult\") appears 2 time.\n",
      "Word 13273 (\"floridians\") appears 1 time.\n",
      "Word 13274 (\"hiaasen\") appears 1 time.\n",
      "Word 13275 (\"hiassen\") appears 1 time.\n",
      "Word 13276 (\"kook\") appears 1 time.\n",
      "Word 13277 (\"kooky\") appears 1 time.\n",
      "Word 13278 (\"oneliners\") appears 1 time.\n",
      "Word 13279 (\"puzzletype\") appears 1 time.\n",
      "Word 13280 (\"raymond\") appears 1 time.\n",
      "Word 13281 (\"switch\") appears 1 time.\n",
      "Word 13282 (\"trucki\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_16 = bow_corpus[1000]\n",
    "\n",
    "for i in range(len(bow_doc_16)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_16[i][0], \n",
    "                                               dict_LoS[bow_doc_16[i][0]], \n",
    "                                                bow_doc_16[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82a924cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.058170846350075585),\n",
      " (1, 0.20658979431076274),\n",
      " (2, 0.13470582454797142),\n",
      " (3, 0.18866944959723134),\n",
      " (4, 0.14792547993539437),\n",
      " (5, 0.18180722542632116),\n",
      " (6, 0.3350746209463065),\n",
      " (7, 0.07799110410422229),\n",
      " (8, 0.23424725839435148),\n",
      " (9, 0.09279891298683662),\n",
      " (10, 0.14056763473523576),\n",
      " (11, 0.1926772918926665),\n",
      " (12, 0.4457146352178454),\n",
      " (13, 0.16366780344474066),\n",
      " (14, 0.26752205075684826),\n",
      " (15, 0.3587645997997213),\n",
      " (16, 0.03207302462866856),\n",
      " (17, 0.09581806201425498),\n",
      " (18, 0.10826187068080412),\n",
      " (19, 0.12500473106131219),\n",
      " (20, 0.30787046822813985),\n",
      " (21, 0.08372251007696865),\n",
      " (22, 0.19116755752482442),\n",
      " (23, 0.07007324301078675)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddcc1857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lda_model and saving to pickle\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lda_model=pickle.load(open('lda_modelE.pkl', 'rb'))\n",
    "    print(\"Reading lda_model from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_model and saving to pickle\")\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dict_LoS, passes=2, workers=2)\n",
    "    pickle.dump(lda_model,open('lda_modelE.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2e4944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(lda_model, bow_corpus, dict_LoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21ed5cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: gensim in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (61.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.18-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numexpr in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\team knowhow\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517): started\n",
      "  Building wheel for pyLDAvis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136904 sha256=ddc14a50d383b9288dd47e0ab659163d1795cc6db069073f4c42c82becc86c24\n",
      "  Stored in directory: c:\\users\\team knowhow\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2959 sha256=bf2ed2523047192fa79bf4e1a98655e3bde46aa07c9541ba8a479b1dd1ed2710\n",
      "  Stored in directory: c:\\users\\team knowhow\\appdata\\local\\pip\\cache\\wheels\\f8\\e0\\3d\\9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.18 pyLDAvis-3.3.1 sklearn-0.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbc42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
