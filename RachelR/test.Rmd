---
title: "Example Assessment Documentation"
author: "Daniel Lawson"
date: "01/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Read the data
```{r}
conndata=read.table("../Lectures/Content/data/conn_sample.log",as.is=T)
colnames(conndata)=c('ts','uid','id.orig_h','id.orig_p',
    'id.resp_h','id.resp_p','proto','service','duration',
    'orig_bytes','resp_bytes','conn_state','local_orig',
    'missed_bytes','history','orig_pkts','orig_ip_bytes',
    'resp_pkts','resp_ip_bytes','tunnel_parents')
```

## Initial exploration

The first step is to examine the data using the material given in class. This starts with an examination of the protocol and service fields, which are likely to be very important types of information about the nature of the information flow.

```{r}
## Making a cyber table
library("knitr")
cyber=table(conndata[,c("proto","service")])
kable(cyber)
```

Since we were asked to visualise it, we'll also plot the relative importance of each service, by protocol.
```{r}
## Barplot
#png("../media/02_EDA_barplot.png",height=500,width=800)
par(mfrow=c(1,3))
for(i in 1:dim(cyber)[1]) barplot(sort(cyber[i,],decreasing=T),
                                  main=rownames(cyber)[i],offset=0.1,
        log="y",col=rainbow(10),las=2)
#dev.off()
```
This makes the importance of the "missing" field very clear. Although we had available an image in the raw data,
```{r}
## Raw image
image(1:dim(cyber)[1],1:dim(cyber)[2],log(1+cyber),
      axes=F,xlab="",ylab="")
axis(1,1:dim(cyber)[1],rownames(cyber),las=2)
axis(2,1:dim(cyber)[2],colnames(cyber),las=2)
```
the normalised version produced by "heatmap" was better:
```{r}
## Using builtin heatmap
heatmap(log(1+cyber),margins =c(6,15)) 
## Rescales rows and columns by d efault
```

The best of all is the heatmap2 version
```{r}
## gplots adds a scale and doesn't scale by default
library("gplots")
#png("../media/02_EDA_heatmap2.png",height=500,width=800)
heatmap.2(log(1+cyber),margins =c(9,15),trace="none")
#dev.off()
```
And this is how we make a fancy table with elements marked-up by content. Note that it will open in a browser when called from regular R sessions, but Rstudio can handle HTML.
```{r}
## datatable creates HTML version of data
library(DT)
cuts=c(1,50,200,1000,10000)
colors=c("white", "lightblue","blue","magenta","purple","red")
datatable(t(unclass(cyber))) %>%
  formatStyle(columns = rownames(cyber), 
              background = styleInterval(cuts,colors))
## You have to screenshot it to get the image
## in the slides.
```


### Durations
Now investigating durations:

Creating a histogram:
```{r}
## Histogram
tcpduration=conndata[conndata[,'proto']=="tcp",'duration']
tcpduration=tcpduration[tcpduration!="-"]
tcpduration=as.numeric(tcpduration)
#png("../media/02_EDA_historgram.png",height=500,width=800)
hist(log(tcpduration),breaks=20,probability=TRUE,col="red")
#dev.off()
```


This next example shows a nice way R is restyled by the DT library: the two lines are equivelent:
```{r}
matrix(head(sort(names(table(tcpduration))),12),
             nrow=3)
names(table(tcpduration)) %>% sort %>% head(n=12) %>% matrix(nrow=3)
```


Two ways to generate ECDFs:
```{r}
## ECDF
ports=conndata[,"id.orig_p"]
## Manually:
#png("../media/02_EDA_port_edf.png",height=500,width=800)
plot(sort(ports),1:length(ports)/length(ports),
  type="s",ylim=c(0,1),
  main = c("Port Frequency Visualisation"),
  sub=("Empiricial Cumulative Distribution Function"),
  xlab=c("Port Number"),ylab=c("cumulative fraction"))
#dev.off()
## Easier:
plot(ecdf(ports))
```

How to generate a survival curve:
```{r}
## Survival
tecdf=ecdf(log(tcpduration)) ## tecdf is a function!
tx=sort(log(unique(tcpduration)))
tsurvival=data.frame(x=exp(tx),y=1-tecdf(tx))
#png("../media/02_EDA_duration_survival.png",height=500,width=800)
plot(tsurvival,type="s",log="xy",
     xlab="Duration (Seconds)",
     ylab="Proportion as long or longer")
#dev.off()
```


And a scatterplot:
```{r}
## Scatterplot
conndata2=conndata[conndata[,"proto"]=="tcp",]
servicefactor=as.factor(conndata2[,'service'])
mycols=rainbow(length(levels(servicefactor)))
mycols[1]="#AAAAAAFF" # RRGGBBAA (Red Green Blue Alpha)
#png("../media/02_EDA_scatterplot.png",height=500,width=800)
plot(conndata2[,'orig_pkts'],conndata2[,'orig_ip_bytes'],
     log="xy",pch=4,col=mycols[servicefactor],
     xlab="Number of packets of data",ylab="Total amount of data")
legend("bottomright",legend=levels(servicefactor),text.col=mycols)
#dev.off()
```


## Missingness

It is clear that missingness is a really important problem! So we'll start with an investigation of that:

```{r}
conndataM=conndata
for(i in c(9,10,11,16:19)) conndataM[,i]=as.numeric(conndataM[,i])
for(i in c(7,8)) conndataM[,i]=as.factor(conndataM[,i])
conndataM2=apply(conndataM,2,function(x)is.na(x) | x=="-")
apply(conndataM2,2,mean)
```

## t-sne

The remit included an examination of other visualisation tools. We found a package, t-sne, that seems to be very popular so tried it on this data.

Starting with missingness

```{r}
library("tsne")
mytsne=tsne(dist(t(mdata)))
plot(mytsne)
```
Oh, thats disappointing! What about the full dataset? We have to do something about missing data. Since we don't know anything about that yet, we'll replace those with 0. We'll treat "missing" in the categories as a special category.

```{r}
conndataM=conndata
conndataM[conndataM[,8]=="-",8]="Missing"
for(i in c(9,10,11,16:19)) {
  conndataM[,i]=as.numeric(conndataM[,i])
  conndataM[is.na(conndataM[,i]),i]=0
}
for(i in c(7,8)) conndataM[,i]=as.factor(conndataM[,i])
```
 Now we can try plotting it
```{r}
## Oh dear, factors don't work in dist. Lets get rid of them
mycols=c("id.orig_p","id.resp_p","duration","orig_bytes","resp_bytes","orig_pkts","orig_ip_bytes")
## Also better standardize
testdata=apply(conndataM[,mycols],2,scale)
mydist=dist(t(testdata))

# This doesn't work, the columns are incompatable
mytsne=tsne((mydist))
plot(mytsne)
```

Not inspiring! We have the wrong sort of data for t-sne to compare columns. We could compare rows though. tsne is quite slow so we'll sample random points
 
```{r}
testpoints=testdata[sample(1:dim(testdata)[1],1000),]
testdist=dist(testpoints)
mytsne=tsne(testdist)
```

Plotting:
```{r}
library("RColorBrewer")
getcolor=function(x){
  colchart=seq(min(x),max(x),length.out=10)
  pal=brewer.pal(9,"RdPu")
  pal[sapply(x,function(y)min(which(colchart>=y))-1)]
}
par(mfrow=c(2,4))
for(i in 1:7) plot(mytsne,col=getcolor(testpoints[,i]),xlab="",ylab="",main=colnames(testpoints)[i])
```
Looks pretty cool!  Not so useful given the scale though.


## Other attempts at visualisation

This resource:

http://www.sthda.com/english/articles/32-r-graphics-essentials/129-visualizing-multivariate-categorical-data/

describes some nice approaches. But we have weird categerorical variables with lots of unique or rare values. The next step is to remove those and put them into a single category.

```{r}
library(FactoMineR)
library(factoextra)
#conndataF=conndataM[,c("id.orig_h","id.resp_h","id.resp_p","proto","service")]
ipportdf=conndataM[, c("id.orig_h","id.resp_p","service")]
# keeping service for later

porttab=table(ipportdf[,2])
commonports=names(porttab)[porttab>100]
ipportdf[!ipportdf[,2]%in%commonports,2]="other"

iptab=table(ipportdf[,1])
commonips=names(iptab)[iptab>100]
ipportdf[!ipportdf[,1]%in%commonips,1]="other"

```
Now we can do an analysis:
```{r}
ipport=as.data.frame(apply(ipportdf,2,as.factor))
mytab=table(ipport[,1:2])
res.ca <- CA(log(1+mytab), graph = FALSE) 
fviz_ca_biplot(res.ca, repel = TRUE) 
```

Oh dear, that didn't work so well on this data.  We can try the balloonplot:

```{r}
library("gplots")
# 1. convert the data as a table
# 2. Graph
balloonplot(t(log(1+mytab)), main ="housetasks", xlab ="", ylab="",
            label = FALSE, show.margins = FALSE)
```
Once again, we find that our table is too big and too sparse. Lets resort to the old favorite, a heatmap
```{r}
heatmap.2(log(1+mytab),margins =c(9,15),trace="none")
```
That is much nicer! We can see he most important ports for each IP quite clearly.
```{r}
portservice=table(ipport[,2:3])
heatmap.2(log(1+portservice),margins =c(9,15),scale="none",trace="none")
```
This shows a very clear correspondance between "port" and "service", with a few ports being important for http, others for dns, others for ssl, etc. Then "everything else" seems to happen in the "missing service" category.
```{r}
par(mfrow=c(3,3))
for(i in 1:9){
  tdata=portservice[portservice[,i]>0,i,drop=FALSE]
  tdata2=tdata[order(tdata[,1]),,drop=FALSE]
  barplot(t(tdata2),
          main=colnames(portservice)[i],
          las=2)
}
```
This seems very reassuring. Internet resources describe "TCP/IP ports" that match these ports (http://www.pearsonitcertification.com/articles/article.aspx?p=1868080). Wikipedia implies that different "protocols" to TCP use different ports (https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers) which explains why a large number of services use "other" ports. Presumably, this is also effected by the nature of the dataset - is it possible that some attacks that were used lie about the service that is running on a given port?

## Graph appraoches

There ar many graph approaches we could try, here is one:

https://briatte.github.io/ggnet/#example-2-bipartite-network

```{r}
library(network)
library(sna)
library(ggplot2)
# set colors for each mode
col = c("actor" = "grey", "event" = "gold")

# weighted adjacency matrix
bipdata = as.matrix(mytab)[-1,-1] # Re
class(bipdata)="matrix"
#bipdata[bipdata>0]=log(1+bipdata[bipdata>0])/10 # make it unweighted
bipdata[bipdata<10]=0 #log(1+bipdata[bipdata>0])/10 # make it unweighted
bipdata[bipdata>0]=1
bipdata=as.data.frame(bipdata)

# weighted bipartite network
bip = network(bipdata,
              matrix.type = "bipartite",
              ignore.eval = FALSE,
              names.eval = "weights")

# detect and color the mode
ggnet2(bip, color = "mode", palette = col, label = TRUE,edge.size = "weights",layout.exp = 0.5,alpha = 0.75)
```
We get several outlying actors (IP addresses) in this view, and a couple of outlying events (ports). They are not the ones that correspond to any specific known behaviour. We get a "hub and spokes" view with most ports in a ring around many IPs. This is hard to interpret, though pretty cool.

Some IP addresses really stick out as "doing a lot of different things".


