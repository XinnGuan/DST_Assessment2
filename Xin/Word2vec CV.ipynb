{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba8d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2a6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('Data/data_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ed7ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[good, helpfull, read, book, good, type, thats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[sadly, overprice, irrelevant, spite, claim, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[endless, rant, howard, borrow, dennis, miller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[not, quite, hip, really, shame, time, reserch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[journey, centre, earth, hey, great, book, abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[star, short, easy, explanation, follow, lot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[michael, hague, illustration, best, buy, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[quotamerican, beautyquot, novel, dark, comedy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[funny, quirky, really, funny, witty, book, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[soldier, bible, old, son, army, buy, bible, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                                               text\n",
       "0        3.0  [good, helpfull, read, book, good, type, thats...\n",
       "1        1.0  [sadly, overprice, irrelevant, spite, claim, i...\n",
       "2        2.0  [endless, rant, howard, borrow, dennis, miller...\n",
       "3        1.0  [not, quite, hip, really, shame, time, reserch...\n",
       "4        5.0  [journey, centre, earth, hey, great, book, abs...\n",
       "...      ...                                                ...\n",
       "49995    5.0  [star, short, easy, explanation, follow, lot, ...\n",
       "49996    5.0  [michael, hague, illustration, best, buy, love...\n",
       "49997    5.0  [quotamerican, beautyquot, novel, dark, comedy...\n",
       "49998    4.0  [funny, quirky, really, funny, witty, book, su...\n",
       "49999    5.0  [soldier, bible, old, son, army, buy, bible, t...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668d8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.2, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df['text'], \n",
    "                                                        df['score'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418cb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "5.0    24226\n",
      "4.0     7728\n",
      "3.0     3422\n",
      "1.0     2638\n",
      "2.0     1986\n",
      "Name: score, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "5.0    6042\n",
      "4.0    1963\n",
      "3.0     854\n",
      "1.0     639\n",
      "2.0     502\n",
      "Name: score, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index                                               text\n",
      "0  45570  [multidimensional, thought, book, good, job, p...\n",
      "1  21855  [magnetic, influence, success, business, find,...\n",
      "2  41127  [read, weep, world, full, bad, book, write, am...\n",
      "3   4849  [clear, channel, watt, book, great, show, book...\n",
      "4  20742  [page, miss, line, mass, market, paperback, ed...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6521f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Data/X_train')\n",
    "X_test.to_pickle('Data/X_test')\n",
    "y_train.to_pickle('Data/y_train')\n",
    "y_test.to_pickle('Data/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504de5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_pickle('Data/X_train')\n",
    "X_test=pd.read_pickle('Data/X_test')\n",
    "y_train=pd.read_pickle('Data/y_train')\n",
    "y_test=pd.read_pickle('Data/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418365c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model:116.67597889900208\n"
     ]
    }
   ],
   "source": [
    "vector_size=1000\n",
    "#The number of dimensions of embeddings, the default is 100\n",
    "window=3\n",
    "#The maximum distance between a target word and its neighbours, default is 5\n",
    "min_count=3\n",
    "#The minimum counts of words in order for the word to be considered for the training of model, default is 5\n",
    "workers=3\n",
    "#The number of partitions during training, default is 3\n",
    "sg=1\n",
    "#The training algorithom, either CBOW(0) or skip gram(1), default is CBOW\n",
    "\n",
    "#tokens = pd.Series(df['tokenized_text']).values\n",
    "start_t = time.time()\n",
    "w2vmodel=Word2Vec(X_train['text'],min_count=min_count,vector_size=vector_size,workers=workers,sg=sg)\n",
    "print('Time taken to train word2vec model:' +str(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6d76a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='w2v_model.joblib'\n",
    "joblib.dump(w2vmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786ff00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=joblib.load('w2v_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a7761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_file = '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/' + 'word2vec_' + '.model'\n",
    "w2vmodel.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc35323",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(word2vec_model_file) #we have to save and load for some of the function below to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d44952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 0,\n",
       " 'not': 1,\n",
       " 'read': 2,\n",
       " 'one': 3,\n",
       " 'story': 4,\n",
       " 'like': 5,\n",
       " 'would': 6,\n",
       " 'time': 7,\n",
       " 'great': 8,\n",
       " 'make': 9,\n",
       " 'good': 10,\n",
       " 'write': 11,\n",
       " 'get': 12,\n",
       " 'find': 13,\n",
       " 'character': 14,\n",
       " 'love': 15,\n",
       " 'life': 16,\n",
       " 'well': 17,\n",
       " 'think': 18,\n",
       " 'novel': 19,\n",
       " 'first': 20,\n",
       " 'many': 21,\n",
       " 'much': 22,\n",
       " 'know': 23,\n",
       " 'work': 24,\n",
       " 'people': 25,\n",
       " 'take': 26,\n",
       " 'also': 27,\n",
       " 'way': 28,\n",
       " 'give': 29,\n",
       " 'even': 30,\n",
       " 'really': 31,\n",
       " 'could': 32,\n",
       " 'year': 33,\n",
       " 'say': 34,\n",
       " 'author': 35,\n",
       " 'want': 36,\n",
       " 'see': 37,\n",
       " 'world': 38,\n",
       " 'end': 39,\n",
       " 'reader': 40,\n",
       " 'come': 41,\n",
       " 'best': 42,\n",
       " 'use': 43,\n",
       " 'thing': 44,\n",
       " 'go': 45,\n",
       " 'new': 46,\n",
       " 'look': 47,\n",
       " 'little': 48,\n",
       " 'page': 49,\n",
       " 'never': 50,\n",
       " 'seem': 51,\n",
       " 'recommend': 52,\n",
       " 'two': 53,\n",
       " 'man': 54,\n",
       " 'enjoy': 55,\n",
       " 'still': 56,\n",
       " 'tell': 57,\n",
       " 'ever': 58,\n",
       " 'child': 59,\n",
       " 'part': 60,\n",
       " 'feel': 61,\n",
       " 'old': 62,\n",
       " 'start': 63,\n",
       " 'live': 64,\n",
       " 'need': 65,\n",
       " 'series': 66,\n",
       " 'woman': 67,\n",
       " 'every': 68,\n",
       " 'try': 69,\n",
       " 'must': 70,\n",
       " 'put': 71,\n",
       " 'back': 72,\n",
       " 'history': 73,\n",
       " 'help': 74,\n",
       " 'point': 75,\n",
       " 'day': 76,\n",
       " 'family': 77,\n",
       " 'keep': 78,\n",
       " 'lot': 79,\n",
       " 'become': 80,\n",
       " 'may': 81,\n",
       " 'understand': 82,\n",
       " 'long': 83,\n",
       " 'another': 84,\n",
       " 'young': 85,\n",
       " 'however': 86,\n",
       " 'anyone': 87,\n",
       " 'war': 88,\n",
       " 'begin': 89,\n",
       " 'learn': 90,\n",
       " 'though': 91,\n",
       " 'interest': 92,\n",
       " 'chapter': 93,\n",
       " 'review': 94,\n",
       " 'word': 95,\n",
       " 'show': 96,\n",
       " 'buy': 97,\n",
       " 'something': 98,\n",
       " 'classic': 99,\n",
       " 'the': 100,\n",
       " 'excellent': 101,\n",
       " 'plot': 102,\n",
       " 'real': 103,\n",
       " 'place': 104,\n",
       " 'different': 105,\n",
       " 'leave': 106,\n",
       " 'fact': 107,\n",
       " 'always': 108,\n",
       " 'bad': 109,\n",
       " 'wonderful': 110,\n",
       " 'without': 111,\n",
       " 'do': 112,\n",
       " 'friend': 113,\n",
       " 'human': 114,\n",
       " 'believe': 115,\n",
       " 'yet': 116,\n",
       " 'right': 117,\n",
       " 'experience': 118,\n",
       " 'change': 119,\n",
       " 'true': 120,\n",
       " 'reading': 121,\n",
       " 'easy': 122,\n",
       " 'school': 123,\n",
       " 'american': 124,\n",
       " 'idea': 125,\n",
       " 'detail': 126,\n",
       " 'set': 127,\n",
       " 'since': 128,\n",
       " 'hard': 129,\n",
       " 'mind': 130,\n",
       " 'happen': 131,\n",
       " 'last': 132,\n",
       " 'turn': 133,\n",
       " 'god': 134,\n",
       " 'quite': 135,\n",
       " 'include': 136,\n",
       " 'far': 137,\n",
       " 'around': 138,\n",
       " 'enough': 139,\n",
       " 'mean': 140,\n",
       " 'movie': 141,\n",
       " 'star': 142,\n",
       " 'person': 143,\n",
       " 'almost': 144,\n",
       " 'name': 145,\n",
       " 'information': 146,\n",
       " 'and': 147,\n",
       " 'follow': 148,\n",
       " 'writer': 149,\n",
       " 'cover': 150,\n",
       " 'call': 151,\n",
       " 'rather': 152,\n",
       " 'highly': 153,\n",
       " 'actually': 154,\n",
       " 'style': 155,\n",
       " 'might': 156,\n",
       " 'age': 157,\n",
       " 'problem': 158,\n",
       " 'high': 159,\n",
       " 'others': 160,\n",
       " 'them': 161,\n",
       " 'view': 162,\n",
       " 'short': 163,\n",
       " 'favorite': 164,\n",
       " 'nothing': 165,\n",
       " 'tale': 166,\n",
       " 'bit': 167,\n",
       " 'away': 168,\n",
       " 'although': 169,\n",
       " 'worth': 170,\n",
       " 'all': 171,\n",
       " 'interesting': 172,\n",
       " 'everyone': 173,\n",
       " 'reason': 174,\n",
       " 'edition': 175,\n",
       " 'men': 176,\n",
       " 'society': 177,\n",
       " 'next': 178,\n",
       " 'everything': 179,\n",
       " 'present': 180,\n",
       " 'sense': 181,\n",
       " 'often': 182,\n",
       " 'whole': 183,\n",
       " 'example': 184,\n",
       " 'three': 185,\n",
       " 'bring': 186,\n",
       " 'especially': 187,\n",
       " 'fun': 188,\n",
       " 'big': 189,\n",
       " 'important': 190,\n",
       " 'course': 191,\n",
       " 'that': 192,\n",
       " 'girl': 193,\n",
       " 'question': 194,\n",
       " 'move': 195,\n",
       " 'lose': 196,\n",
       " 'main': 197,\n",
       " 'this': 198,\n",
       " 'fan': 199,\n",
       " 'action': 200,\n",
       " 'adventure': 201,\n",
       " 'provide': 202,\n",
       " 'play': 203,\n",
       " 'create': 204,\n",
       " 'finish': 205,\n",
       " 'state': 206,\n",
       " 'father': 207,\n",
       " 'second': 208,\n",
       " 'truly': 209,\n",
       " 'today': 210,\n",
       " 'several': 211,\n",
       " 'kind': 212,\n",
       " 'job': 213,\n",
       " 'let': 214,\n",
       " 'able': 215,\n",
       " 'fiction': 216,\n",
       " 'you': 217,\n",
       " 'order': 218,\n",
       " 'study': 219,\n",
       " 'probably': 220,\n",
       " 'someone': 221,\n",
       " 'describe': 222,\n",
       " 'language': 223,\n",
       " 'beautiful': 224,\n",
       " 'felt': 225,\n",
       " 'home': 226,\n",
       " 'along': 227,\n",
       " 'event': 228,\n",
       " 'lead': 229,\n",
       " 'early': 230,\n",
       " 'sure': 231,\n",
       " 'deal': 232,\n",
       " 'version': 233,\n",
       " 'him': 234,\n",
       " 'again': 235,\n",
       " 'simply': 236,\n",
       " 'least': 237,\n",
       " 'full': 238,\n",
       " 'text': 239,\n",
       " 'be': 240,\n",
       " 'case': 241,\n",
       " 'anything': 242,\n",
       " 'heart': 243,\n",
       " 'picture': 244,\n",
       " 'death': 245,\n",
       " 'small': 246,\n",
       " 'expect': 247,\n",
       " 'class': 248,\n",
       " 'miss': 249,\n",
       " 'thought': 250,\n",
       " 'pick': 251,\n",
       " 'student': 252,\n",
       " 'description': 253,\n",
       " 'talk': 254,\n",
       " 'mystery': 255,\n",
       " 'century': 256,\n",
       " 'mother': 257,\n",
       " 'line': 258,\n",
       " 'less': 259,\n",
       " 'subject': 260,\n",
       " 'english': 261,\n",
       " 'meet': 262,\n",
       " 'ago': 263,\n",
       " 'explain': 264,\n",
       " 'money': 265,\n",
       " 'hand': 266,\n",
       " 'together': 267,\n",
       " 'modern': 268,\n",
       " 'hope': 269,\n",
       " 'power': 270,\n",
       " 'past': 271,\n",
       " 'boy': 272,\n",
       " 'perhaps': 273,\n",
       " 'relationship': 274,\n",
       " 'kid': 275,\n",
       " 'copy': 276,\n",
       " 'wish': 277,\n",
       " 'later': 278,\n",
       " 'consider': 279,\n",
       " 'literature': 280,\n",
       " 'john': 281,\n",
       " 'can': 282,\n",
       " 'down': 283,\n",
       " 'grow': 284,\n",
       " 'instead': 285,\n",
       " 'fall': 286,\n",
       " 'care': 287,\n",
       " 'matter': 288,\n",
       " 'pretty': 289,\n",
       " 'interested': 290,\n",
       " 'simple': 291,\n",
       " 'historical': 292,\n",
       " 'title': 293,\n",
       " 'force': 294,\n",
       " 'base': 295,\n",
       " 'run': 296,\n",
       " 'kill': 297,\n",
       " 'science': 298,\n",
       " 'wait': 299,\n",
       " 'issue': 300,\n",
       " 'personal': 301,\n",
       " 'open': 302,\n",
       " 'christian': 303,\n",
       " 'perfect': 304,\n",
       " 'eye': 305,\n",
       " 'reference': 306,\n",
       " 'hold': 307,\n",
       " 'clear': 308,\n",
       " 'already': 309,\n",
       " 'original': 310,\n",
       " 'teach': 311,\n",
       " 'die': 312,\n",
       " 'guide': 313,\n",
       " 'funny': 314,\n",
       " 'truth': 315,\n",
       " 'note': 316,\n",
       " 'definitely': 317,\n",
       " 'throughout': 318,\n",
       " 'out': 319,\n",
       " 'face': 320,\n",
       " 'scene': 321,\n",
       " 'wonder': 322,\n",
       " 'difficult': 323,\n",
       " 'son': 324,\n",
       " 'add': 325,\n",
       " 'black': 326,\n",
       " 'side': 327,\n",
       " 'daughter': 328,\n",
       " 'insight': 329,\n",
       " 'wrong': 330,\n",
       " 'realize': 331,\n",
       " 'nature': 332,\n",
       " 'jane': 333,\n",
       " 'romance': 334,\n",
       " 'form': 335,\n",
       " 'publish': 336,\n",
       " 'finally': 337,\n",
       " 'house': 338,\n",
       " 'continue': 339,\n",
       " 'country': 340,\n",
       " 'parent': 341,\n",
       " 'strong': 342,\n",
       " 'art': 343,\n",
       " 'fantasy': 344,\n",
       " 'future': 345,\n",
       " 'social': 346,\n",
       " 'certainly': 347,\n",
       " 'nice': 348,\n",
       " 'king': 349,\n",
       " 'lack': 350,\n",
       " 'amaze': 351,\n",
       " 'else': 352,\n",
       " 'level': 353,\n",
       " 'mention': 354,\n",
       " 'sometimes': 355,\n",
       " 'maybe': 356,\n",
       " 'culture': 357,\n",
       " 'focus': 358,\n",
       " 'there': 359,\n",
       " 'research': 360,\n",
       " 'complete': 361,\n",
       " 'wife': 362,\n",
       " 'struggle': 363,\n",
       " 'light': 364,\n",
       " 'ask': 365,\n",
       " 'decide': 366,\n",
       " 'offer': 367,\n",
       " 'opinion': 368,\n",
       " 'remember': 369,\n",
       " 'group': 370,\n",
       " 'account': 371,\n",
       " 'piece': 372,\n",
       " 'theme': 373,\n",
       " 'deep': 374,\n",
       " 'close': 375,\n",
       " 'list': 376,\n",
       " 'attempt': 377,\n",
       " 'hear': 378,\n",
       " 'her': 379,\n",
       " 'bible': 380,\n",
       " 'lord': 381,\n",
       " 'section': 382,\n",
       " 'number': 383,\n",
       " 'type': 384,\n",
       " 'america': 385,\n",
       " 'quality': 386,\n",
       " 'within': 387,\n",
       " 'return': 388,\n",
       " 'upon': 389,\n",
       " 'etc': 390,\n",
       " 'gift': 391,\n",
       " 'more': 392,\n",
       " 'white': 393,\n",
       " 'purchase': 394,\n",
       " 'hero': 395,\n",
       " 'plan': 396,\n",
       " 'free': 397,\n",
       " 'late': 398,\n",
       " 'large': 399,\n",
       " 'political': 400,\n",
       " 'stand': 401,\n",
       " 'collection': 402,\n",
       " 'allow': 403,\n",
       " 'entire': 404,\n",
       " 'stop': 405,\n",
       " 'night': 406,\n",
       " 'basic': 407,\n",
       " 'now': 408,\n",
       " 'sister': 409,\n",
       " 'completely': 410,\n",
       " 'happy': 411,\n",
       " 'either': 412,\n",
       " 'journey': 413,\n",
       " 'rest': 414,\n",
       " 'here': 415,\n",
       " 'head': 416,\n",
       " 'result': 417,\n",
       " 'middle': 418,\n",
       " 'half': 419,\n",
       " 'fine': 420,\n",
       " 'travel': 421,\n",
       " 'guy': 422,\n",
       " 'four': 423,\n",
       " 'develop': 424,\n",
       " 'share': 425,\n",
       " 'major': 426,\n",
       " 'evil': 427,\n",
       " 'entertain': 428,\n",
       " 'fascinate': 429,\n",
       " 'save': 430,\n",
       " 'murder': 431,\n",
       " 'value': 432,\n",
       " 'dark': 433,\n",
       " 'answer': 434,\n",
       " 'fill': 435,\n",
       " 'draw': 436,\n",
       " 'situation': 437,\n",
       " 'business': 438,\n",
       " 'concept': 439,\n",
       " 'battle': 440,\n",
       " 'knowledge': 441,\n",
       " 'general': 442,\n",
       " 'theory': 443,\n",
       " 'material': 444,\n",
       " 'philosophy': 445,\n",
       " 'dream': 446,\n",
       " 'appear': 447,\n",
       " 'fight': 448,\n",
       " 'period': 449,\n",
       " 'adult': 450,\n",
       " 'system': 451,\n",
       " 'enjoyable': 452,\n",
       " 'brother': 453,\n",
       " 'figure': 454,\n",
       " 'perspective': 455,\n",
       " 'touch': 456,\n",
       " 'reviewer': 457,\n",
       " 'city': 458,\n",
       " 'spend': 459,\n",
       " 'feeling': 460,\n",
       " 'too': 461,\n",
       " 'individual': 462,\n",
       " 'five': 463,\n",
       " 'volume': 464,\n",
       " 'ring': 465,\n",
       " 'remain': 466,\n",
       " 'rich': 467,\n",
       " 'speak': 468,\n",
       " 'library': 469,\n",
       " 'cause': 470,\n",
       " 'introduction': 471,\n",
       " 'extremely': 472,\n",
       " 'humor': 473,\n",
       " 'moment': 474,\n",
       " 'couple': 475,\n",
       " 'whether': 476,\n",
       " 'overall': 477,\n",
       " 'powerful': 478,\n",
       " 'church': 479,\n",
       " 'stay': 480,\n",
       " 'teacher': 481,\n",
       " 'absolutely': 482,\n",
       " 'sort': 483,\n",
       " 'sound': 484,\n",
       " 'laugh': 485,\n",
       " 'agree': 486,\n",
       " 'various': 487,\n",
       " 'poor': 488,\n",
       " 'husband': 489,\n",
       " 'relate': 490,\n",
       " 'surprise': 491,\n",
       " 'development': 492,\n",
       " 'earth': 493,\n",
       " 'aspect': 494,\n",
       " 'soon': 495,\n",
       " 'but': 496,\n",
       " 'certain': 497,\n",
       " 'involve': 498,\n",
       " 'term': 499,\n",
       " 'bore': 500,\n",
       " 'act': 501,\n",
       " 'clearly': 502,\n",
       " 'behind': 503,\n",
       " 'attention': 504,\n",
       " 'town': 505,\n",
       " 'among': 506,\n",
       " 'suggest': 507,\n",
       " 'across': 508,\n",
       " 'game': 509,\n",
       " 'easily': 510,\n",
       " 'useful': 511,\n",
       " 'topic': 512,\n",
       " 'animal': 513,\n",
       " 'build': 514,\n",
       " 'land': 515,\n",
       " 'approach': 516,\n",
       " 'twist': 517,\n",
       " 'watch': 518,\n",
       " 'search': 519,\n",
       " 'condition': 520,\n",
       " 'emotion': 521,\n",
       " 'message': 522,\n",
       " 'particularly': 523,\n",
       " 'discover': 524,\n",
       " 'literary': 525,\n",
       " 'religion': 526,\n",
       " 'price': 527,\n",
       " 'break': 528,\n",
       " 'austen': 529,\n",
       " 'require': 530,\n",
       " 'effort': 531,\n",
       " 'reality': 532,\n",
       " 'control': 533,\n",
       " 'excite': 534,\n",
       " 'unique': 535,\n",
       " 'film': 536,\n",
       " 'yes': 537,\n",
       " 'saw': 538,\n",
       " 'contain': 539,\n",
       " 'george': 540,\n",
       " 'appreciate': 541,\n",
       " 'ability': 542,\n",
       " 'quotthe': 543,\n",
       " 'somewhat': 544,\n",
       " 'third': 545,\n",
       " 'government': 546,\n",
       " 'helpful': 547,\n",
       " 'month': 548,\n",
       " 'claim': 549,\n",
       " 'reread': 550,\n",
       " 'narrative': 551,\n",
       " 'despite': 552,\n",
       " 'master': 553,\n",
       " 'step': 554,\n",
       " 'beyond': 555,\n",
       " 'depth': 556,\n",
       " 'process': 557,\n",
       " 'possible': 558,\n",
       " 'challenge': 559,\n",
       " 'faith': 560,\n",
       " 'secret': 561,\n",
       " 'forward': 562,\n",
       " 'pay': 563,\n",
       " 'whose': 564,\n",
       " 'lesson': 565,\n",
       " 'top': 566,\n",
       " 'source': 567,\n",
       " 'letter': 568,\n",
       " 'choose': 569,\n",
       " 'slow': 570,\n",
       " 'brilliant': 571,\n",
       " 'fantastic': 572,\n",
       " 'capture': 573,\n",
       " 'serious': 574,\n",
       " 'role': 575,\n",
       " 'practice': 576,\n",
       " 'law': 577,\n",
       " 'please': 578,\n",
       " 'receive': 579,\n",
       " 'stuff': 580,\n",
       " 'particular': 581,\n",
       " 'kindle': 582,\n",
       " 'choice': 583,\n",
       " 'alone': 584,\n",
       " 'guess': 585,\n",
       " 'bear': 586,\n",
       " 'print': 587,\n",
       " 'common': 588,\n",
       " 'complex': 589,\n",
       " 'listen': 590,\n",
       " 'rule': 591,\n",
       " 'fear': 592,\n",
       " 'voice': 593,\n",
       " 'content': 594,\n",
       " 'moral': 595,\n",
       " 'dog': 596,\n",
       " 'area': 597,\n",
       " 'sad': 598,\n",
       " 'fast': 599,\n",
       " 'body': 600,\n",
       " 'quick': 601,\n",
       " 'forget': 602,\n",
       " 'fail': 603,\n",
       " 'marry': 604,\n",
       " 'about': 605,\n",
       " 'exactly': 606,\n",
       " 'prose': 607,\n",
       " 'illustration': 608,\n",
       " 'support': 609,\n",
       " 'single': 610,\n",
       " 'college': 611,\n",
       " 'soul': 612,\n",
       " 'jesus': 613,\n",
       " 'quickly': 614,\n",
       " 'catch': 615,\n",
       " 'week': 616,\n",
       " 'regard': 617,\n",
       " 'for': 618,\n",
       " 'writing': 619,\n",
       " 'background': 620,\n",
       " 'dead': 621,\n",
       " 'sit': 622,\n",
       " 'hour': 623,\n",
       " 'date': 624,\n",
       " 'food': 625,\n",
       " 'lewis': 626,\n",
       " 'color': 627,\n",
       " 'pass': 628,\n",
       " 'previous': 629,\n",
       " 'sex': 630,\n",
       " 'effect': 631,\n",
       " 'crime': 632,\n",
       " 'design': 633,\n",
       " 'hobbit': 634,\n",
       " 'indeed': 635,\n",
       " 'have': 636,\n",
       " 'amazon': 637,\n",
       " 'engage': 638,\n",
       " 'beauty': 639,\n",
       " 'final': 640,\n",
       " 'compare': 641,\n",
       " 'rise': 642,\n",
       " 'spirit': 643,\n",
       " 'inspire': 644,\n",
       " 'tolkien': 645,\n",
       " 'own': 646,\n",
       " 'imagine': 647,\n",
       " 'mark': 648,\n",
       " 'island': 649,\n",
       " 'marriage': 650,\n",
       " 'totally': 651,\n",
       " 'usually': 652,\n",
       " 'seek': 653,\n",
       " 'with': 654,\n",
       " 'member': 655,\n",
       " 'doubt': 656,\n",
       " 'glad': 657,\n",
       " 'religious': 658,\n",
       " 'strange': 659,\n",
       " 'due': 660,\n",
       " 'waste': 661,\n",
       " 'disappointed': 662,\n",
       " 'visit': 663,\n",
       " 'james': 664,\n",
       " 'chance': 665,\n",
       " 'special': 666,\n",
       " 'harry': 667,\n",
       " 'memory': 668,\n",
       " 'masterpiece': 669,\n",
       " 'serve': 670,\n",
       " 'recipe': 671,\n",
       " 'conclusion': 672,\n",
       " 'desire': 673,\n",
       " 'magic': 674,\n",
       " 'british': 675,\n",
       " 'reach': 676,\n",
       " 'available': 677,\n",
       " 'respect': 678,\n",
       " 'military': 679,\n",
       " 'company': 680,\n",
       " 'test': 681,\n",
       " 'send': 682,\n",
       " 'famous': 683,\n",
       " 'walk': 684,\n",
       " 'lie': 685,\n",
       " 'suppose': 686,\n",
       " 'public': 687,\n",
       " 'england': 688,\n",
       " 'similar': 689,\n",
       " 'trouble': 690,\n",
       " 'conflict': 691,\n",
       " 'check': 692,\n",
       " 'field': 693,\n",
       " 'hate': 694,\n",
       " 'advice': 695,\n",
       " 'difference': 696,\n",
       " 'exist': 697,\n",
       " 'program': 698,\n",
       " 'music': 699,\n",
       " 'intrigue': 700,\n",
       " 'which': 701,\n",
       " 'element': 702,\n",
       " 'self': 703,\n",
       " 'raise': 704,\n",
       " 'suffer': 705,\n",
       " 'mostly': 706,\n",
       " 'treat': 707,\n",
       " 'concern': 708,\n",
       " 'race': 709,\n",
       " 'escape': 710,\n",
       " 'purpose': 711,\n",
       " 'dialogue': 712,\n",
       " 'admit': 713,\n",
       " 'survive': 714,\n",
       " 'thus': 715,\n",
       " 'drive': 716,\n",
       " 'inside': 717,\n",
       " 'addition': 718,\n",
       " 'trip': 719,\n",
       " 'nearly': 720,\n",
       " 'sentence': 721,\n",
       " 'discuss': 722,\n",
       " 'thank': 723,\n",
       " 'awesome': 724,\n",
       " 'towards': 725,\n",
       " 'image': 726,\n",
       " 'then': 727,\n",
       " 'lover': 728,\n",
       " 'manage': 729,\n",
       " 'carry': 730,\n",
       " 'spiritual': 731,\n",
       " 'accept': 732,\n",
       " 'horror': 733,\n",
       " 'current': 734,\n",
       " 'belief': 735,\n",
       " 'incredible': 736,\n",
       " 'ship': 737,\n",
       " 'translation': 738,\n",
       " 'eventually': 739,\n",
       " 'western': 740,\n",
       " 'generation': 741,\n",
       " 'quote': 742,\n",
       " 'romantic': 743,\n",
       " 'baby': 744,\n",
       " 'genre': 745,\n",
       " 'pain': 746,\n",
       " 'argument': 747,\n",
       " 'understanding': 748,\n",
       " 'comment': 749,\n",
       " 'prove': 750,\n",
       " 'amp': 751,\n",
       " 'fit': 752,\n",
       " 'unfortunately': 753,\n",
       " 'reveal': 754,\n",
       " 'standard': 755,\n",
       " 'amazing': 756,\n",
       " 'resource': 757,\n",
       " 'paper': 758,\n",
       " 'childhood': 759,\n",
       " 'discussion': 760,\n",
       " 'analysis': 761,\n",
       " 'german': 762,\n",
       " 'huge': 763,\n",
       " 'community': 764,\n",
       " 'anne': 765,\n",
       " 'remind': 766,\n",
       " 'room': 767,\n",
       " 'hop': 768,\n",
       " 'robert': 769,\n",
       " 'encounter': 770,\n",
       " 'suspense': 771,\n",
       " 'amount': 772,\n",
       " 'explanation': 773,\n",
       " 'imagination': 774,\n",
       " 'party': 775,\n",
       " 'space': 776,\n",
       " 'familiar': 777,\n",
       " 'other': 778,\n",
       " 'train': 779,\n",
       " 'evidence': 780,\n",
       " 'emotional': 781,\n",
       " 'himself': 782,\n",
       " 'actual': 783,\n",
       " 'key': 784,\n",
       " 'female': 785,\n",
       " 'realistic': 786,\n",
       " 'disappoint': 787,\n",
       " 'except': 788,\n",
       " 'service': 789,\n",
       " 'recently': 790,\n",
       " 'personality': 791,\n",
       " 'decision': 792,\n",
       " 'introduce': 793,\n",
       " 'hit': 794,\n",
       " 'red': 795,\n",
       " 'pull': 796,\n",
       " 'influence': 797,\n",
       " 'era': 798,\n",
       " 'method': 799,\n",
       " 'produce': 800,\n",
       " 'compel': 801,\n",
       " 'hundred': 802,\n",
       " 'cry': 803,\n",
       " 'product': 804,\n",
       " 'smith': 805,\n",
       " 'none': 806,\n",
       " 'explore': 807,\n",
       " 'biography': 808,\n",
       " 'french': 809,\n",
       " 'local': 810,\n",
       " 'attack': 811,\n",
       " 'career': 812,\n",
       " 'eat': 813,\n",
       " 'success': 814,\n",
       " 'confuse': 815,\n",
       " 'technique': 816,\n",
       " 'fire': 817,\n",
       " 'ancient': 818,\n",
       " 'christ': 819,\n",
       " 'near': 820,\n",
       " 'joy': 821,\n",
       " 'fully': 822,\n",
       " 'grade': 823,\n",
       " 'pace': 824,\n",
       " 'paint': 825,\n",
       " 'york': 826,\n",
       " 'lady': 827,\n",
       " 'win': 828,\n",
       " 'before': 829,\n",
       " 'popular': 830,\n",
       " 'practical': 831,\n",
       " 'mistake': 832,\n",
       " 'summer': 833,\n",
       " 'flaw': 834,\n",
       " 'boring': 835,\n",
       " 'jack': 836,\n",
       " 'charm': 837,\n",
       " 'christmas': 838,\n",
       " 'obvious': 839,\n",
       " 'manner': 840,\n",
       " 'gain': 841,\n",
       " 'natural': 842,\n",
       " 'feature': 843,\n",
       " 'honest': 844,\n",
       " 'politics': 845,\n",
       " 'tool': 846,\n",
       " 'structure': 847,\n",
       " 'henry': 848,\n",
       " 'water': 849,\n",
       " 'trilogy': 850,\n",
       " 'informative': 851,\n",
       " 'sea': 852,\n",
       " 'pleasure': 853,\n",
       " 'sell': 854,\n",
       " 'leader': 855,\n",
       " 'education': 856,\n",
       " 'beautifully': 857,\n",
       " 'fly': 858,\n",
       " 'cut': 859,\n",
       " 'heroine': 860,\n",
       " 'ten': 861,\n",
       " 'david': 862,\n",
       " 'market': 863,\n",
       " 'arrive': 864,\n",
       " 'thoroughly': 865,\n",
       " 'freedom': 866,\n",
       " 'obviously': 867,\n",
       " 'recent': 868,\n",
       " 'deserve': 869,\n",
       " 'low': 870,\n",
       " 'itself': 871,\n",
       " 'appeal': 872,\n",
       " 'passage': 873,\n",
       " 'south': 874,\n",
       " 'express': 875,\n",
       " 'narrator': 876,\n",
       " 'alive': 877,\n",
       " 'tom': 878,\n",
       " 'skill': 879,\n",
       " 'technology': 880,\n",
       " 'outside': 881,\n",
       " 'essay': 882,\n",
       " 'throw': 883,\n",
       " 'horse': 884,\n",
       " 'intelligent': 885,\n",
       " 'universe': 886,\n",
       " 'rand': 887,\n",
       " 'protagonist': 888,\n",
       " 'unlike': 889,\n",
       " 'strength': 890,\n",
       " 'civil': 891,\n",
       " 'project': 892,\n",
       " 'tragedy': 893,\n",
       " 'artist': 894,\n",
       " 'christianity': 895,\n",
       " 'map': 896,\n",
       " 'west': 897,\n",
       " 'mary': 898,\n",
       " 'toward': 899,\n",
       " 'likely': 900,\n",
       " 'wild': 901,\n",
       " 'genius': 902,\n",
       " 'represent': 903,\n",
       " 'hat': 904,\n",
       " 'nation': 905,\n",
       " 'model': 906,\n",
       " 'poetry': 907,\n",
       " 'error': 908,\n",
       " 'who': 909,\n",
       " 'destroy': 910,\n",
       " 'thriller': 911,\n",
       " 'lay': 912,\n",
       " 'avoid': 913,\n",
       " 'longer': 914,\n",
       " 'position': 915,\n",
       " 'address': 916,\n",
       " 'loss': 917,\n",
       " 'flow': 918,\n",
       " 'road': 919,\n",
       " 'fairly': 920,\n",
       " 'detailed': 921,\n",
       " 'personally': 922,\n",
       " 'record': 923,\n",
       " 'typical': 924,\n",
       " 'principle': 925,\n",
       " 'professional': 926,\n",
       " 'apply': 927,\n",
       " 'essential': 928,\n",
       " 'trust': 929,\n",
       " 'behavior': 930,\n",
       " 'treasure': 931,\n",
       " 'tradition': 932,\n",
       " 'stick': 933,\n",
       " 'former': 934,\n",
       " 'intend': 935,\n",
       " 'movement': 936,\n",
       " 'doctor': 937,\n",
       " 'soldier': 938,\n",
       " 'epic': 939,\n",
       " 'professor': 940,\n",
       " 'cold': 941,\n",
       " 'meaning': 942,\n",
       " 'paul': 943,\n",
       " 'negative': 944,\n",
       " 'indian': 945,\n",
       " 'accurate': 946,\n",
       " 'forever': 947,\n",
       " 'scientific': 948,\n",
       " 'ultimately': 949,\n",
       " 'necessary': 950,\n",
       " 'format': 951,\n",
       " 'successful': 952,\n",
       " 'michael': 953,\n",
       " 'deeply': 954,\n",
       " 'dickens': 955,\n",
       " 'army': 956,\n",
       " 'prejudice': 957,\n",
       " 'notice': 958,\n",
       " 'usual': 959,\n",
       " 'impact': 960,\n",
       " 'center': 961,\n",
       " 'london': 962,\n",
       " 'positive': 963,\n",
       " 'thanks': 964,\n",
       " 'lee': 965,\n",
       " 'brain': 966,\n",
       " 'foundation': 967,\n",
       " 'otherwise': 968,\n",
       " 'emma': 969,\n",
       " 'cook': 970,\n",
       " 'male': 971,\n",
       " 'sequel': 972,\n",
       " 'earlier': 973,\n",
       " 'basically': 974,\n",
       " 'river': 975,\n",
       " 'promise': 976,\n",
       " 'shock': 977,\n",
       " 'humanity': 978,\n",
       " 'strike': 979,\n",
       " 'portrait': 980,\n",
       " 'peter': 981,\n",
       " 'wisdom': 982,\n",
       " 'car': 983,\n",
       " 'immediately': 984,\n",
       " 'outstanding': 985,\n",
       " 'occur': 986,\n",
       " 'stephen': 987,\n",
       " 'average': 988,\n",
       " 'therefore': 989,\n",
       " 'portray': 990,\n",
       " 'terrible': 991,\n",
       " 'photo': 992,\n",
       " 'vision': 993,\n",
       " 'storyline': 994,\n",
       " 'planet': 995,\n",
       " 'becomes': 996,\n",
       " 'passion': 997,\n",
       " 'why': 998,\n",
       " 'physical': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29fbc5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41595, 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>0.093191</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>0.163666</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>-0.092321</td>\n",
       "      <td>0.107138</td>\n",
       "      <td>0.077283</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>-0.051570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104670</td>\n",
       "      <td>-0.083952</td>\n",
       "      <td>0.090864</td>\n",
       "      <td>-0.004854</td>\n",
       "      <td>0.125868</td>\n",
       "      <td>-0.046788</td>\n",
       "      <td>-0.042210</td>\n",
       "      <td>-0.113314</td>\n",
       "      <td>0.046125</td>\n",
       "      <td>0.017304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.049468</td>\n",
       "      <td>0.058147</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.039899</td>\n",
       "      <td>-0.090384</td>\n",
       "      <td>-0.056956</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>0.028193</td>\n",
       "      <td>-0.007900</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.023309</td>\n",
       "      <td>-0.061541</td>\n",
       "      <td>-0.132283</td>\n",
       "      <td>-0.060790</td>\n",
       "      <td>-0.084970</td>\n",
       "      <td>-0.008647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read</th>\n",
       "      <td>0.062747</td>\n",
       "      <td>-0.082553</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>0.248178</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>-0.028716</td>\n",
       "      <td>0.053239</td>\n",
       "      <td>0.043143</td>\n",
       "      <td>-0.069352</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068876</td>\n",
       "      <td>0.031968</td>\n",
       "      <td>0.130032</td>\n",
       "      <td>0.059487</td>\n",
       "      <td>0.113676</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>-0.054621</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.044061</td>\n",
       "      <td>-0.073668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.067632</td>\n",
       "      <td>0.057678</td>\n",
       "      <td>-0.024377</td>\n",
       "      <td>0.147654</td>\n",
       "      <td>-0.089571</td>\n",
       "      <td>-0.070021</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>-0.023408</td>\n",
       "      <td>0.111963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088833</td>\n",
       "      <td>-0.080763</td>\n",
       "      <td>0.078768</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.041459</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>-0.085874</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>-0.070100</td>\n",
       "      <td>-0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.107059</td>\n",
       "      <td>-0.128003</td>\n",
       "      <td>-0.032419</td>\n",
       "      <td>0.046873</td>\n",
       "      <td>0.065347</td>\n",
       "      <td>-0.158554</td>\n",
       "      <td>0.062178</td>\n",
       "      <td>-0.002045</td>\n",
       "      <td>0.053513</td>\n",
       "      <td>-0.029064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023479</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>-0.031661</td>\n",
       "      <td>0.067833</td>\n",
       "      <td>0.147765</td>\n",
       "      <td>0.069098</td>\n",
       "      <td>-0.063918</td>\n",
       "      <td>-0.178700</td>\n",
       "      <td>-0.087061</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "book   0.093191  0.017452  0.033683  0.163666  0.019230 -0.092321  0.107138   \n",
       "not    0.049468  0.058147  0.046993  0.039899 -0.090384 -0.056956  0.097288   \n",
       "read   0.062747 -0.082553  0.035017  0.248178  0.018828 -0.028716  0.053239   \n",
       "one    0.067632  0.057678 -0.024377  0.147654 -0.089571 -0.070021  0.079438   \n",
       "story  0.107059 -0.128003 -0.032419  0.046873  0.065347 -0.158554  0.062178   \n",
       "\n",
       "            7         8         9    ...       990       991       992  \\\n",
       "book   0.077283  0.011580 -0.051570  ... -0.104670 -0.083952  0.090864   \n",
       "not    0.028193 -0.007900  0.016054  ...  0.028335 -0.059975  0.085465   \n",
       "read   0.043143 -0.069352  0.036031  ... -0.068876  0.031968  0.130032   \n",
       "one    0.109028 -0.023408  0.111963  ...  0.088833 -0.080763  0.078768   \n",
       "story -0.002045  0.053513 -0.029064  ... -0.023479  0.018656 -0.031661   \n",
       "\n",
       "            993       994       995       996       997       998       999  \n",
       "book  -0.004854  0.125868 -0.046788 -0.042210 -0.113314  0.046125  0.017304  \n",
       "not    0.098323  0.023309 -0.061541 -0.132283 -0.060790 -0.084970 -0.008647  \n",
       "read   0.059487  0.113676  0.007758 -0.054621 -0.017854  0.044061 -0.073668  \n",
       "one    0.057274  0.041459 -0.002300 -0.085874 -0.046879 -0.070100 -0.023518  \n",
       "story  0.067833  0.147765  0.069098 -0.063918 -0.178700 -0.087061  0.004072  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [w2v_model.wv.get_vector(str(n)) for n in w2vmodel.wv.key_to_index],\n",
    "        index = w2vmodel.wv.key_to_index\n",
    "    )\n",
    ")\n",
    "print(emb_df.shape)\n",
    "\n",
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5823884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    \n",
    "    if nwords ==0:\n",
    "        featureVec = np.zeros(1000)\n",
    "        #avoiding divide by zero, define it as a all zero vector\n",
    "    else:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # Dividing the result by number of words to get average\n",
    "\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a5418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Comment %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "            \n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a8f4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 0 of 40000\n",
      "Comment 1000 of 40000\n",
      "Comment 2000 of 40000\n",
      "Comment 3000 of 40000\n",
      "Comment 4000 of 40000\n",
      "Comment 5000 of 40000\n",
      "Comment 6000 of 40000\n",
      "Comment 7000 of 40000\n",
      "Comment 8000 of 40000\n",
      "Comment 9000 of 40000\n",
      "Comment 10000 of 40000\n",
      "Comment 11000 of 40000\n",
      "Comment 12000 of 40000\n",
      "Comment 13000 of 40000\n",
      "Comment 14000 of 40000\n",
      "Comment 15000 of 40000\n",
      "Comment 16000 of 40000\n",
      "Comment 17000 of 40000\n",
      "Comment 18000 of 40000\n",
      "Comment 19000 of 40000\n",
      "Comment 20000 of 40000\n",
      "Comment 21000 of 40000\n",
      "Comment 22000 of 40000\n",
      "Comment 23000 of 40000\n",
      "Comment 24000 of 40000\n",
      "Comment 25000 of 40000\n",
      "Comment 26000 of 40000\n",
      "Comment 27000 of 40000\n",
      "Comment 28000 of 40000\n",
      "Comment 29000 of 40000\n",
      "Comment 30000 of 40000\n",
      "Comment 31000 of 40000\n",
      "Comment 32000 of 40000\n",
      "Comment 33000 of 40000\n",
      "Comment 34000 of 40000\n",
      "Comment 35000 of 40000\n",
      "Comment 36000 of 40000\n",
      "Comment 37000 of 40000\n",
      "Comment 38000 of 40000\n",
      "Comment 39000 of 40000\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(X_train['text'], w2v_model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91e581c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45570</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21855</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41127</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4849</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>39995</td>\n",
       "      <td>39296</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>39996</td>\n",
       "      <td>49015</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>39997</td>\n",
       "      <td>2693</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>39998</td>\n",
       "      <td>8076</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>39999</td>\n",
       "      <td>7624</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index  score\n",
       "0            0  45570    4.0\n",
       "1            1  21855    5.0\n",
       "2            2  41127    1.0\n",
       "3            3   4849    5.0\n",
       "4            4  20742    1.0\n",
       "...        ...    ...    ...\n",
       "39995    39995  39296    5.0\n",
       "39996    39996  49015    5.0\n",
       "39997    39997   2693    5.0\n",
       "39998    39998   8076    5.0\n",
       "39999    39999   7624    5.0\n",
       "\n",
       "[40000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unuseful_rows(trainDataVecs,y_train):\n",
    "    list=[]\n",
    "    for i in range(trainDataVecs.shape[0]):\n",
    "        if np.all(trainDataVecs[i,]==0):\n",
    "            list.append(i)\n",
    "    if len(list)!=0:\n",
    "        trainDataVecs=np.delete(trainDataVecs,list,axis=0)\n",
    "        y_train.drop(index=list,inplace=True)\n",
    "    return trainDataVecs,y_train\n",
    "# remove the sample with all zero vector and its corresponding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb1500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1,y_train1=remove_unuseful_rows(trainDataVecs,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71810fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39999, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e056193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39999, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32d21f",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d21fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "forest1 = RandomForestClassifier(n_estimators = 10)\n",
    "forest2 = RandomForestClassifier(n_estimators = 100)\n",
    "forest3 = RandomForestClassifier(n_estimators = 500)\n",
    "forest4 = RandomForestClassifier(n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ed105b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 349, in _score\n",
      "    raise ValueError(\"{0} format is not supported\".format(y_type))\n",
      "ValueError: multiclass format is not supported\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 349, in _score\n",
      "    raise ValueError(\"{0} format is not supported\".format(y_type))\n",
      "ValueError: multiclass format is not supported\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 349, in _score\n",
      "    raise ValueError(\"{0} format is not supported\".format(y_type))\n",
      "ValueError: multiclass format is not supported\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 349, in _score\n",
      "    raise ValueError(\"{0} format is not supported\".format(y_type))\n",
      "ValueError: multiclass format is not supported\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Users/xin/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 349, in _score\n",
      "    raise ValueError(\"{0} format is not supported\".format(y_type))\n",
      "ValueError: multiclass format is not supported\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores1 = cross_val_score(forest1, trainDataVecs1, y_train1['score'], scoring='roc_auc', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ce8770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e405a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fit the model\n",
    "forest2 = forest2.fit(trainDataVecs1, y_train1[\"score\"])\n",
    "print(\"Time taken to fit the random forest model 2 with word2vec vectors: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5f7b7",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size=1000\n",
    "window=3\n",
    "min_count=3\n",
    "workers=3\n",
    "sg=1\n",
    "\n",
    "start_t = time.time()\n",
    "test_w2vmodel=Word2Vec(X_test['text'],min_count=min_count,vector_size=vector_size,workers=workers,sg=sg)\n",
    "print('Time taken to test word2vec model:' +str(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='test_w2v_model.joblib'\n",
    "joblib.dump(test_w2vmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2vmodel=joblib.load('test_w2v_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word2vec_model_file = '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/' + 'test_word2vec_' + '.model'\n",
    "test_w2vmodel.save(test_word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25600045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v_model = Word2Vec.load(test_word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [test_w2v_model.wv.get_vector(str(n)) for n in test_w2vmodel.wv.key_to_index],\n",
    "        index = test_w2vmodel.wv.key_to_index\n",
    "    )\n",
    ")\n",
    "print(test_emb_df.shape)\n",
    "\n",
    "test_emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = getAvgFeatureVecs(X_test['text'], test_w2v_model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5123fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs1,y_test1=remove_unuseful_rows(testDataVecs,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=forest.predict(testDataVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(prediction-y_test1['score'],prediction-y_test1['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction-y_test1['score']==0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
