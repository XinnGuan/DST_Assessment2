{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fba8d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30b5a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://builtin.com/machine-learning/nlp-word2vec-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f2a6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('Data/data_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05ed7ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[good, helpfull, read, book, good, type, thats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[sadly, overprice, irrelevant, spite, claim, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[endless, rant, howard, borrow, dennis, miller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[not, quite, hip, really, shame, time, reserch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[journey, centre, earth, hey, great, book, abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[star, short, easy, explanation, follow, lot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[michael, hague, illustration, best, buy, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[quotamerican, beautyquot, novel, dark, comedy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[funny, quirky, really, funny, witty, book, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[soldier, bible, old, son, army, buy, bible, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                                               text\n",
       "0        3.0  [good, helpfull, read, book, good, type, thats...\n",
       "1        1.0  [sadly, overprice, irrelevant, spite, claim, i...\n",
       "2        2.0  [endless, rant, howard, borrow, dennis, miller...\n",
       "3        1.0  [not, quite, hip, really, shame, time, reserch...\n",
       "4        5.0  [journey, centre, earth, hey, great, book, abs...\n",
       "...      ...                                                ...\n",
       "49995    5.0  [star, short, easy, explanation, follow, lot, ...\n",
       "49996    5.0  [michael, hague, illustration, best, buy, love...\n",
       "49997    5.0  [quotamerican, beautyquot, novel, dark, comedy...\n",
       "49998    4.0  [funny, quirky, really, funny, witty, book, su...\n",
       "49999    5.0  [soldier, bible, old, son, army, buy, bible, t...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3796ee85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadly',\n",
       " 'overprice',\n",
       " 'irrelevant',\n",
       " 'spite',\n",
       " 'claim',\n",
       " '``',\n",
       " 'illustrate',\n",
       " 'pmo',\n",
       " 'effective',\n",
       " 'reducing',\n",
       " 'cost',\n",
       " 'project',\n",
       " 'decrease',\n",
       " 'time',\n",
       " 'market',\n",
       " 'new',\n",
       " 'product',\n",
       " 'increase',\n",
       " 'corporate',\n",
       " 'profit',\n",
       " 'ensure',\n",
       " 'project',\n",
       " 'success',\n",
       " \"''\",\n",
       " 'small',\n",
       " 'overpriced',\n",
       " 'book',\n",
       " 'actually',\n",
       " 'none',\n",
       " 'things.a',\n",
       " 'collection',\n",
       " 'unrelated',\n",
       " 'article',\n",
       " 'write',\n",
       " 'dry',\n",
       " 'overly',\n",
       " 'academic',\n",
       " 'style',\n",
       " 'book',\n",
       " 'totally',\n",
       " 'fail',\n",
       " 'address',\n",
       " 'proposition',\n",
       " 'project',\n",
       " 'management',\n",
       " 'office',\n",
       " 'pmo',\n",
       " 'add',\n",
       " 'value',\n",
       " 'organization',\n",
       " 'many',\n",
       " 'statement',\n",
       " 'effect',\n",
       " '``',\n",
       " 'the',\n",
       " 'pmo',\n",
       " '...',\n",
       " \"''\",\n",
       " 'perform',\n",
       " 'function',\n",
       " 'assertion',\n",
       " 'function',\n",
       " 'itself',\n",
       " 'whether',\n",
       " 'easily',\n",
       " 'perform',\n",
       " 'organization',\n",
       " 'not',\n",
       " 'formal',\n",
       " 'pmo.it',\n",
       " 'surprising',\n",
       " 'half',\n",
       " 'eight',\n",
       " 'chapter',\n",
       " 'relationship',\n",
       " 'pmo',\n",
       " 'function',\n",
       " 'all',\n",
       " 'discus',\n",
       " 'various',\n",
       " 'project',\n",
       " 'management',\n",
       " 'issue',\n",
       " 'interesting',\n",
       " 'well',\n",
       " 'cover',\n",
       " 'elsewhere',\n",
       " 'literature',\n",
       " 'topic',\n",
       " 'not',\n",
       " 'one',\n",
       " 'expect',\n",
       " 'purchase',\n",
       " 'book',\n",
       " 'purportedly',\n",
       " 'discuss',\n",
       " 'pmo',\n",
       " 'function.the',\n",
       " 'book',\n",
       " 'completely',\n",
       " 'devoid',\n",
       " 'real',\n",
       " 'world',\n",
       " 'example',\n",
       " 'leave',\n",
       " 'author',\n",
       " 'assertion',\n",
       " 'efficacy',\n",
       " 'idea',\n",
       " 'also',\n",
       " 'paucity',\n",
       " 'real',\n",
       " 'action',\n",
       " 'item',\n",
       " 'chapter',\n",
       " '``',\n",
       " 'implementing',\n",
       " 'pmo',\n",
       " \"''\",\n",
       " 'ob',\n",
       " 'group',\n",
       " 'report',\n",
       " 'organization',\n",
       " 'practical',\n",
       " 'step-by-step',\n",
       " 'advice',\n",
       " 'actually',\n",
       " 'create',\n",
       " 'pmo',\n",
       " 'chapter',\n",
       " 'do',\n",
       " 'however',\n",
       " 'contain',\n",
       " 'interesting',\n",
       " 'albeit',\n",
       " 'irrelevant',\n",
       " 'discussion',\n",
       " 'manage',\n",
       " '``',\n",
       " 'project',\n",
       " 'virtual',\n",
       " 'team',\n",
       " '``',\n",
       " 'a',\n",
       " 'someone',\n",
       " 'charge',\n",
       " 'actually',\n",
       " 'create',\n",
       " 'work',\n",
       " 'pmo',\n",
       " 'large',\n",
       " 'organization',\n",
       " 'extremely',\n",
       " 'disappointed',\n",
       " 'book',\n",
       " 'offer',\n",
       " 'almost',\n",
       " 'practical',\n",
       " 'information',\n",
       " 'topic']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668d8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.2, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df['text'], \n",
    "                                                        df['score'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418cb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "5.0    24226\n",
      "4.0     7728\n",
      "3.0     3422\n",
      "1.0     2638\n",
      "2.0     1986\n",
      "Name: score, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "5.0    6042\n",
      "4.0    1963\n",
      "3.0     854\n",
      "1.0     639\n",
      "2.0     502\n",
      "Name: score, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index                                               text\n",
      "0  45570  [multidimensional, thought, book, good, job, p...\n",
      "1  21855  [magnetic, influence, success, business, find,...\n",
      "2  41127  [read, weep, world, full, bad, book, write, am...\n",
      "3   4849  [clear, channel, watt, book, great, show, book...\n",
      "4  20742  [page, miss, line, mass, market, paperback, ed...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6521f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Data/X_train')\n",
    "X_test.to_pickle('Data/X_test')\n",
    "y_train.to_pickle('Data/y_train')\n",
    "y_test.to_pickle('Data/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504de5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_pickle('Data/X_train')\n",
    "X_test=pd.read_pickle('Data/X_test')\n",
    "y_train=pd.read_pickle('Data/y_train')\n",
    "y_test=pd.read_pickle('Data/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418365c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model:116.67597889900208\n"
     ]
    }
   ],
   "source": [
    "vector_size=1000\n",
    "#The number of dimensions of embeddings, the default is 100\n",
    "window=3\n",
    "#The maximum distance between a target word and its neighbours, default is 5\n",
    "min_count=3\n",
    "#The minimum counts of words in order for the word to be considered for the training of model, default is 5\n",
    "workers=3\n",
    "#The number of partitions during training, default is 3\n",
    "sg=1\n",
    "#The training algorithom, either CBOW(0) or skip gram(1), default is CBOW\n",
    "\n",
    "#tokens = pd.Series(df['tokenized_text']).values\n",
    "start_t = time.time()\n",
    "w2vmodel=Word2Vec(X_train['text'],min_count=min_count,vector_size=vector_size,workers=workers,sg=sg)\n",
    "print('Time taken to train word2vec model:' +str(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6d76a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='w2v_model.joblib'\n",
    "joblib.dump(w2vmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786ff00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=joblib.load('w2v_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a7761d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out: '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/word2vec_.model.syn1neg.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/xfpwvv6x0fg3pkqm99zxr6z40000gq/T/ipykernel_68141/2589231946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword2vec_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'word2vec_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         \"\"\"\n\u001b[0;32m-> 1900\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         restores = self._save_specials(\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_save_specials\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[0;31m# don't save properties that are merely calculated from others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cum_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m         return super(Word2Vec, self)._save_specials(\n\u001b[0m\u001b[1;32m   1907\u001b[0m             fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\n\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_save_specials\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[1;32m    680\u001b[0m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out: '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/word2vec_.model.syn1neg.npy'"
     ]
    }
   ],
   "source": [
    "word2vec_model_file = '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/' + 'word2vec_' + '.model'\n",
    "w2vmodel.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc35323",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(word2vec_model_file) #we have to save and load for some of the function below to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d44952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 0,\n",
       " 'not': 1,\n",
       " 'read': 2,\n",
       " 'one': 3,\n",
       " 'story': 4,\n",
       " 'like': 5,\n",
       " 'would': 6,\n",
       " 'time': 7,\n",
       " 'great': 8,\n",
       " 'make': 9,\n",
       " 'good': 10,\n",
       " 'get': 11,\n",
       " 'write': 12,\n",
       " 'character': 13,\n",
       " 'find': 14,\n",
       " 'love': 15,\n",
       " 'life': 16,\n",
       " 'well': 17,\n",
       " 'think': 18,\n",
       " 'novel': 19,\n",
       " 'first': 20,\n",
       " 'many': 21,\n",
       " 'know': 22,\n",
       " 'much': 23,\n",
       " 'work': 24,\n",
       " 'people': 25,\n",
       " 'take': 26,\n",
       " 'way': 27,\n",
       " 'also': 28,\n",
       " 'give': 29,\n",
       " 'even': 30,\n",
       " 'really': 31,\n",
       " 'could': 32,\n",
       " 'year': 33,\n",
       " 'say': 34,\n",
       " 'author': 35,\n",
       " 'want': 36,\n",
       " 'see': 37,\n",
       " 'world': 38,\n",
       " 'end': 39,\n",
       " 'reader': 40,\n",
       " 'come': 41,\n",
       " 'best': 42,\n",
       " 'use': 43,\n",
       " 'thing': 44,\n",
       " 'go': 45,\n",
       " 'new': 46,\n",
       " 'look': 47,\n",
       " 'little': 48,\n",
       " 'never': 49,\n",
       " 'page': 50,\n",
       " 'seem': 51,\n",
       " 'two': 52,\n",
       " 'recommend': 53,\n",
       " 'man': 54,\n",
       " 'enjoy': 55,\n",
       " 'tell': 56,\n",
       " 'still': 57,\n",
       " 'ever': 58,\n",
       " 'child': 59,\n",
       " 'part': 60,\n",
       " 'feel': 61,\n",
       " 'live': 62,\n",
       " 'old': 63,\n",
       " 'start': 64,\n",
       " 'need': 65,\n",
       " 'series': 66,\n",
       " 'woman': 67,\n",
       " 'every': 68,\n",
       " 'try': 69,\n",
       " 'put': 70,\n",
       " 'must': 71,\n",
       " 'back': 72,\n",
       " 'help': 73,\n",
       " 'history': 74,\n",
       " 'day': 75,\n",
       " 'lot': 76,\n",
       " 'family': 77,\n",
       " 'point': 78,\n",
       " 'keep': 79,\n",
       " 'become': 80,\n",
       " 'may': 81,\n",
       " 'young': 82,\n",
       " 'long': 83,\n",
       " 'another': 84,\n",
       " 'understand': 85,\n",
       " 'war': 86,\n",
       " 'however': 87,\n",
       " 'anyone': 88,\n",
       " 'begin': 89,\n",
       " 'chapter': 90,\n",
       " 'though': 91,\n",
       " 'interest': 92,\n",
       " 'learn': 93,\n",
       " 'word': 94,\n",
       " 'review': 95,\n",
       " 'buy': 96,\n",
       " 'classic': 97,\n",
       " 'show': 98,\n",
       " 'the': 99,\n",
       " 'something': 100,\n",
       " 'plot': 101,\n",
       " 'real': 102,\n",
       " 'place': 103,\n",
       " 'excellent': 104,\n",
       " 'different': 105,\n",
       " 'fact': 106,\n",
       " 'leave': 107,\n",
       " 'always': 108,\n",
       " 'bad': 109,\n",
       " 'wonderful': 110,\n",
       " 'friend': 111,\n",
       " 'without': 112,\n",
       " 'do': 113,\n",
       " 'human': 114,\n",
       " 'yet': 115,\n",
       " 'right': 116,\n",
       " 'believe': 117,\n",
       " 'experience': 118,\n",
       " 'change': 119,\n",
       " 'true': 120,\n",
       " 'easy': 121,\n",
       " 'reading': 122,\n",
       " 'school': 123,\n",
       " 'american': 124,\n",
       " 'detail': 125,\n",
       " 'idea': 126,\n",
       " 'set': 127,\n",
       " 'since': 128,\n",
       " 'hard': 129,\n",
       " 'happen': 130,\n",
       " 'mind': 131,\n",
       " 'turn': 132,\n",
       " 'last': 133,\n",
       " 'god': 134,\n",
       " 'quite': 135,\n",
       " 'include': 136,\n",
       " 'around': 137,\n",
       " 'far': 138,\n",
       " 'movie': 139,\n",
       " 'enough': 140,\n",
       " 'star': 141,\n",
       " 'mean': 142,\n",
       " 'almost': 143,\n",
       " 'person': 144,\n",
       " 'and': 145,\n",
       " 'information': 146,\n",
       " 'writer': 147,\n",
       " 'name': 148,\n",
       " 'cover': 149,\n",
       " 'call': 150,\n",
       " 'follow': 151,\n",
       " 'style': 152,\n",
       " 'rather': 153,\n",
       " 'problem': 154,\n",
       " 'high': 155,\n",
       " 'highly': 156,\n",
       " 'might': 157,\n",
       " 'actually': 158,\n",
       " 'others': 159,\n",
       " 'age': 160,\n",
       " 'them': 161,\n",
       " 'view': 162,\n",
       " 'nothing': 163,\n",
       " 'short': 164,\n",
       " 'tale': 165,\n",
       " 'favorite': 166,\n",
       " 'away': 167,\n",
       " 'worth': 168,\n",
       " 'although': 169,\n",
       " 'bit': 170,\n",
       " 'everyone': 171,\n",
       " 'reason': 172,\n",
       " 'interesting': 173,\n",
       " 'men': 174,\n",
       " 'all': 175,\n",
       " 'everything': 176,\n",
       " 'next': 177,\n",
       " 'edition': 178,\n",
       " 'society': 179,\n",
       " 'present': 180,\n",
       " 'sense': 181,\n",
       " 'especially': 182,\n",
       " 'whole': 183,\n",
       " 'big': 184,\n",
       " 'often': 185,\n",
       " 'fun': 186,\n",
       " 'example': 187,\n",
       " 'three': 188,\n",
       " 'move': 189,\n",
       " 'bring': 190,\n",
       " 'that': 191,\n",
       " 'course': 192,\n",
       " 'lose': 193,\n",
       " 'important': 194,\n",
       " 'fan': 195,\n",
       " 'this': 196,\n",
       " 'main': 197,\n",
       " 'play': 198,\n",
       " 'adventure': 199,\n",
       " 'question': 200,\n",
       " 'action': 201,\n",
       " 'girl': 202,\n",
       " 'create': 203,\n",
       " 'provide': 204,\n",
       " 'truly': 205,\n",
       " 'finish': 206,\n",
       " 'second': 207,\n",
       " 'state': 208,\n",
       " 'job': 209,\n",
       " 'let': 210,\n",
       " 'father': 211,\n",
       " 'order': 212,\n",
       " 'you': 213,\n",
       " 'kind': 214,\n",
       " 'today': 215,\n",
       " 'study': 216,\n",
       " 'several': 217,\n",
       " 'probably': 218,\n",
       " 'describe': 219,\n",
       " 'able': 220,\n",
       " 'felt': 221,\n",
       " 'beautiful': 222,\n",
       " 'event': 223,\n",
       " 'him': 224,\n",
       " 'home': 225,\n",
       " 'fiction': 226,\n",
       " 'someone': 227,\n",
       " 'version': 228,\n",
       " 'language': 229,\n",
       " 'deal': 230,\n",
       " 'along': 231,\n",
       " 'sure': 232,\n",
       " 'anything': 233,\n",
       " 'simply': 234,\n",
       " 'lead': 235,\n",
       " 'case': 236,\n",
       " 'again': 237,\n",
       " 'picture': 238,\n",
       " 'description': 239,\n",
       " 'early': 240,\n",
       " 'death': 241,\n",
       " 'be': 242,\n",
       " 'small': 243,\n",
       " 'least': 244,\n",
       " 'text': 245,\n",
       " 'full': 246,\n",
       " 'expect': 247,\n",
       " 'pick': 248,\n",
       " 'heart': 249,\n",
       " 'mystery': 250,\n",
       " 'miss': 251,\n",
       " 'talk': 252,\n",
       " 'mother': 253,\n",
       " 'class': 254,\n",
       " 'thought': 255,\n",
       " 'student': 256,\n",
       " 'less': 257,\n",
       " 'explain': 258,\n",
       " 'meet': 259,\n",
       " 'money': 260,\n",
       " 'english': 261,\n",
       " 'line': 262,\n",
       " 'century': 263,\n",
       " 'hand': 264,\n",
       " 'ago': 265,\n",
       " 'modern': 266,\n",
       " 'subject': 267,\n",
       " 'boy': 268,\n",
       " 'together': 269,\n",
       " 'hope': 270,\n",
       " 'relationship': 271,\n",
       " 'past': 272,\n",
       " 'later': 273,\n",
       " 'power': 274,\n",
       " 'kid': 275,\n",
       " 'perhaps': 276,\n",
       " 'wish': 277,\n",
       " 'copy': 278,\n",
       " 'literature': 279,\n",
       " 'john': 280,\n",
       " 'down': 281,\n",
       " 'consider': 282,\n",
       " 'instead': 283,\n",
       " 'grow': 284,\n",
       " 'can': 285,\n",
       " 'pretty': 286,\n",
       " 'fall': 287,\n",
       " 'care': 288,\n",
       " 'interested': 289,\n",
       " 'title': 290,\n",
       " 'matter': 291,\n",
       " 'simple': 292,\n",
       " 'historical': 293,\n",
       " 'run': 294,\n",
       " 'kill': 295,\n",
       " 'base': 296,\n",
       " 'issue': 297,\n",
       " 'force': 298,\n",
       " 'perfect': 299,\n",
       " 'wait': 300,\n",
       " 'eye': 301,\n",
       " 'personal': 302,\n",
       " 'science': 303,\n",
       " 'clear': 304,\n",
       " 'open': 305,\n",
       " 'already': 306,\n",
       " 'reference': 307,\n",
       " 'hold': 308,\n",
       " 'die': 309,\n",
       " 'scene': 310,\n",
       " 'truth': 311,\n",
       " 'guide': 312,\n",
       " 'difficult': 313,\n",
       " 'christian': 314,\n",
       " 'definitely': 315,\n",
       " 'face': 316,\n",
       " 'funny': 317,\n",
       " 'throughout': 318,\n",
       " 'strong': 319,\n",
       " 'jane': 320,\n",
       " 'teach': 321,\n",
       " 'wonder': 322,\n",
       " 'out': 323,\n",
       " 'note': 324,\n",
       " 'fantasy': 325,\n",
       " 'finally': 326,\n",
       " 'original': 327,\n",
       " 'wrong': 328,\n",
       " 'daughter': 329,\n",
       " 'parent': 330,\n",
       " 'form': 331,\n",
       " 'realize': 332,\n",
       " 'side': 333,\n",
       " 'black': 334,\n",
       " 'nature': 335,\n",
       " 'country': 336,\n",
       " 'son': 337,\n",
       " 'continue': 338,\n",
       " 'insight': 339,\n",
       " 'publish': 340,\n",
       " 'romance': 341,\n",
       " 'future': 342,\n",
       " 'house': 343,\n",
       " 'art': 344,\n",
       " 'add': 345,\n",
       " 'nice': 346,\n",
       " 'certainly': 347,\n",
       " 'king': 348,\n",
       " 'else': 349,\n",
       " 'amaze': 350,\n",
       " 'social': 351,\n",
       " 'focus': 352,\n",
       " 'maybe': 353,\n",
       " 'lack': 354,\n",
       " 'research': 355,\n",
       " 'level': 356,\n",
       " 'wife': 357,\n",
       " 'sometimes': 358,\n",
       " 'remember': 359,\n",
       " 'culture': 360,\n",
       " 'there': 361,\n",
       " 'mention': 362,\n",
       " 'deep': 363,\n",
       " 'complete': 364,\n",
       " 'group': 365,\n",
       " 'struggle': 366,\n",
       " 'opinion': 367,\n",
       " 'bible': 368,\n",
       " 'her': 369,\n",
       " 'piece': 370,\n",
       " 'light': 371,\n",
       " 'number': 372,\n",
       " 'america': 373,\n",
       " 'account': 374,\n",
       " 'offer': 375,\n",
       " 'attempt': 376,\n",
       " 'theme': 377,\n",
       " 'hear': 378,\n",
       " 'decide': 379,\n",
       " 'close': 380,\n",
       " 'section': 381,\n",
       " 'ask': 382,\n",
       " 'lord': 383,\n",
       " 'return': 384,\n",
       " 'list': 385,\n",
       " 'upon': 386,\n",
       " 'quality': 387,\n",
       " 'more': 388,\n",
       " 'white': 389,\n",
       " 'type': 390,\n",
       " 'allow': 391,\n",
       " 'hero': 392,\n",
       " 'political': 393,\n",
       " 'free': 394,\n",
       " 'etc': 395,\n",
       " 'collection': 396,\n",
       " 'gift': 397,\n",
       " 'within': 398,\n",
       " 'journey': 399,\n",
       " 'purchase': 400,\n",
       " 'late': 401,\n",
       " 'night': 402,\n",
       " 'stand': 403,\n",
       " 'happy': 404,\n",
       " 'large': 405,\n",
       " 'now': 406,\n",
       " 'head': 407,\n",
       " 'sister': 408,\n",
       " 'stop': 409,\n",
       " 'plan': 410,\n",
       " 'half': 411,\n",
       " 'entire': 412,\n",
       " 'result': 413,\n",
       " 'basic': 414,\n",
       " 'guy': 415,\n",
       " 'rest': 416,\n",
       " 'travel': 417,\n",
       " 'either': 418,\n",
       " 'middle': 419,\n",
       " 'here': 420,\n",
       " 'completely': 421,\n",
       " 'theory': 422,\n",
       " 'four': 423,\n",
       " 'fascinate': 424,\n",
       " 'evil': 425,\n",
       " 'develop': 426,\n",
       " 'entertain': 427,\n",
       " 'fine': 428,\n",
       " 'save': 429,\n",
       " 'dark': 430,\n",
       " 'fill': 431,\n",
       " 'value': 432,\n",
       " 'draw': 433,\n",
       " 'share': 434,\n",
       " 'knowledge': 435,\n",
       " 'situation': 436,\n",
       " 'dream': 437,\n",
       " 'material': 438,\n",
       " 'general': 439,\n",
       " 'murder': 440,\n",
       " 'figure': 441,\n",
       " 'brother': 442,\n",
       " 'battle': 443,\n",
       " 'whether': 444,\n",
       " 'system': 445,\n",
       " 'perspective': 446,\n",
       " 'period': 447,\n",
       " 'concept': 448,\n",
       " 'adult': 449,\n",
       " 'too': 450,\n",
       " 'moment': 451,\n",
       " 'enjoyable': 452,\n",
       " 'spend': 453,\n",
       " 'cause': 454,\n",
       " 'city': 455,\n",
       " 'appear': 456,\n",
       " 'ring': 457,\n",
       " 'major': 458,\n",
       " 'answer': 459,\n",
       " 'speak': 460,\n",
       " 'remain': 461,\n",
       " 'five': 462,\n",
       " 'feeling': 463,\n",
       " 'volume': 464,\n",
       " 'couple': 465,\n",
       " 'philosophy': 466,\n",
       " 'touch': 467,\n",
       " 'business': 468,\n",
       " 'introduction': 469,\n",
       " 'extremely': 470,\n",
       " 'rich': 471,\n",
       " 'fight': 472,\n",
       " 'powerful': 473,\n",
       " 'husband': 474,\n",
       " 'library': 475,\n",
       " 'absolutely': 476,\n",
       " 'reviewer': 477,\n",
       " 'sort': 478,\n",
       " 'surprise': 479,\n",
       " 'bore': 480,\n",
       " 'overall': 481,\n",
       " 'sound': 482,\n",
       " 'certain': 483,\n",
       " 'agree': 484,\n",
       " 'various': 485,\n",
       " 'aspect': 486,\n",
       " 'earth': 487,\n",
       " 'humor': 488,\n",
       " 'relate': 489,\n",
       " 'stay': 490,\n",
       " 'development': 491,\n",
       " 'church': 492,\n",
       " 'laugh': 493,\n",
       " 'individual': 494,\n",
       " 'teacher': 495,\n",
       " 'involve': 496,\n",
       " 'poor': 497,\n",
       " 'watch': 498,\n",
       " 'attention': 499,\n",
       " 'game': 500,\n",
       " 'soon': 501,\n",
       " 'town': 502,\n",
       " 'but': 503,\n",
       " 'across': 504,\n",
       " 'useful': 505,\n",
       " 'act': 506,\n",
       " 'topic': 507,\n",
       " 'behind': 508,\n",
       " 'easily': 509,\n",
       " 'austen': 510,\n",
       " 'among': 511,\n",
       " 'particularly': 512,\n",
       " 'animal': 513,\n",
       " 'discover': 514,\n",
       " 'message': 515,\n",
       " 'term': 516,\n",
       " 'control': 517,\n",
       " 'condition': 518,\n",
       " 'religion': 519,\n",
       " 'build': 520,\n",
       " 'literary': 521,\n",
       " 'twist': 522,\n",
       " 'emotion': 523,\n",
       " 'land': 524,\n",
       " 'government': 525,\n",
       " 'suggest': 526,\n",
       " 'approach': 527,\n",
       " 'break': 528,\n",
       " 'yes': 529,\n",
       " 'search': 530,\n",
       " 'reality': 531,\n",
       " 'film': 532,\n",
       " 'clearly': 533,\n",
       " 'require': 534,\n",
       " 'quotthe': 535,\n",
       " 'appreciate': 536,\n",
       " 'price': 537,\n",
       " 'ability': 538,\n",
       " 'excite': 539,\n",
       " 'george': 540,\n",
       " 'claim': 541,\n",
       " 'whose': 542,\n",
       " 'effort': 543,\n",
       " 'saw': 544,\n",
       " 'unique': 545,\n",
       " 'despite': 546,\n",
       " 'third': 547,\n",
       " 'forward': 548,\n",
       " 'month': 549,\n",
       " 'narrative': 550,\n",
       " 'slow': 551,\n",
       " 'helpful': 552,\n",
       " 'challenge': 553,\n",
       " 'somewhat': 554,\n",
       " 'contain': 555,\n",
       " 'serious': 556,\n",
       " 'beyond': 557,\n",
       " 'possible': 558,\n",
       " 'process': 559,\n",
       " 'step': 560,\n",
       " 'secret': 561,\n",
       " 'reread': 562,\n",
       " 'pay': 563,\n",
       " 'depth': 564,\n",
       " 'master': 565,\n",
       " 'choice': 566,\n",
       " 'lesson': 567,\n",
       " 'top': 568,\n",
       " 'letter': 569,\n",
       " 'capture': 570,\n",
       " 'fantastic': 571,\n",
       " 'law': 572,\n",
       " 'source': 573,\n",
       " 'moral': 574,\n",
       " 'kindle': 575,\n",
       " 'bear': 576,\n",
       " 'receive': 577,\n",
       " 'please': 578,\n",
       " 'brilliant': 579,\n",
       " 'practice': 580,\n",
       " 'complex': 581,\n",
       " 'choose': 582,\n",
       " 'alone': 583,\n",
       " 'guess': 584,\n",
       " 'common': 585,\n",
       " 'role': 586,\n",
       " 'faith': 587,\n",
       " 'listen': 588,\n",
       " 'particular': 589,\n",
       " 'body': 590,\n",
       " 'print': 591,\n",
       " 'stuff': 592,\n",
       " 'single': 593,\n",
       " 'sad': 594,\n",
       " 'jesus': 595,\n",
       " 'voice': 596,\n",
       " 'area': 597,\n",
       " 'fast': 598,\n",
       " 'fear': 599,\n",
       " 'rule': 600,\n",
       " 'for': 601,\n",
       " 'content': 602,\n",
       " 'soul': 603,\n",
       " 'prose': 604,\n",
       " 'quick': 605,\n",
       " 'marry': 606,\n",
       " 'regard': 607,\n",
       " 'lewis': 608,\n",
       " 'forget': 609,\n",
       " 'writing': 610,\n",
       " 'catch': 611,\n",
       " 'date': 612,\n",
       " 'week': 613,\n",
       " 'fail': 614,\n",
       " 'sex': 615,\n",
       " 'pass': 616,\n",
       " 'exactly': 617,\n",
       " 'college': 618,\n",
       " 'quickly': 619,\n",
       " 'sit': 620,\n",
       " 'support': 621,\n",
       " 'island': 622,\n",
       " 'illustration': 623,\n",
       " 'design': 624,\n",
       " 'background': 625,\n",
       " 'about': 626,\n",
       " 'effect': 627,\n",
       " 'final': 628,\n",
       " 'dead': 629,\n",
       " 'dog': 630,\n",
       " 'hour': 631,\n",
       " 'crime': 632,\n",
       " 'previous': 633,\n",
       " 'totally': 634,\n",
       " 'compare': 635,\n",
       " 'color': 636,\n",
       " 'rise': 637,\n",
       " 'amazon': 638,\n",
       " 'indeed': 639,\n",
       " 'inspire': 640,\n",
       " 'beauty': 641,\n",
       " 'glad': 642,\n",
       " 'usually': 643,\n",
       " 'have': 644,\n",
       " 'marriage': 645,\n",
       " 'engage': 646,\n",
       " 'hobbit': 647,\n",
       " 'mark': 648,\n",
       " 'own': 649,\n",
       " 'spirit': 650,\n",
       " 'religious': 651,\n",
       " 'serve': 652,\n",
       " 'imagine': 653,\n",
       " 'seek': 654,\n",
       " 'food': 655,\n",
       " 'doubt': 656,\n",
       " 'company': 657,\n",
       " 'tolkien': 658,\n",
       " 'strange': 659,\n",
       " 'visit': 660,\n",
       " 'disappointed': 661,\n",
       " 'magic': 662,\n",
       " 'harry': 663,\n",
       " 'military': 664,\n",
       " 'member': 665,\n",
       " 'with': 666,\n",
       " 'james': 667,\n",
       " 'lie': 668,\n",
       " 'memory': 669,\n",
       " 'chance': 670,\n",
       " 'due': 671,\n",
       " 'conclusion': 672,\n",
       " 'waste': 673,\n",
       " 'send': 674,\n",
       " 'special': 675,\n",
       " 'reach': 676,\n",
       " 'walk': 677,\n",
       " 'masterpiece': 678,\n",
       " 'desire': 679,\n",
       " 'similar': 680,\n",
       " 'respect': 681,\n",
       " 'british': 682,\n",
       " 'music': 683,\n",
       " 'available': 684,\n",
       " 'famous': 685,\n",
       " 'recipe': 686,\n",
       " 'difference': 687,\n",
       " 'conflict': 688,\n",
       " 'self': 689,\n",
       " 'suppose': 690,\n",
       " 'program': 691,\n",
       " 'public': 692,\n",
       " 'exist': 693,\n",
       " 'hate': 694,\n",
       " 'test': 695,\n",
       " 'intrigue': 696,\n",
       " 'element': 697,\n",
       " 'escape': 698,\n",
       " 'sentence': 699,\n",
       " 'check': 700,\n",
       " 'advice': 701,\n",
       " 'field': 702,\n",
       " 'concern': 703,\n",
       " 'dialogue': 704,\n",
       " 'trouble': 705,\n",
       " 'raise': 706,\n",
       " 'mostly': 707,\n",
       " 'which': 708,\n",
       " 'spiritual': 709,\n",
       " 'survive': 710,\n",
       " 'addition': 711,\n",
       " 'suffer': 712,\n",
       " 'discuss': 713,\n",
       " 'admit': 714,\n",
       " 'england': 715,\n",
       " 'image': 716,\n",
       " 'nearly': 717,\n",
       " 'horror': 718,\n",
       " 'inside': 719,\n",
       " 'romantic': 720,\n",
       " 'argument': 721,\n",
       " 'thus': 722,\n",
       " 'accept': 723,\n",
       " 'carry': 724,\n",
       " 'treat': 725,\n",
       " 'race': 726,\n",
       " 'purpose': 727,\n",
       " 'pain': 728,\n",
       " 'thank': 729,\n",
       " 'manage': 730,\n",
       " 'lover': 731,\n",
       " 'current': 732,\n",
       " 'awesome': 733,\n",
       " 'ship': 734,\n",
       " 'fit': 735,\n",
       " 'drive': 736,\n",
       " 'towards': 737,\n",
       " 'baby': 738,\n",
       " 'room': 739,\n",
       " 'eventually': 740,\n",
       " 'amazing': 741,\n",
       " 'amp': 742,\n",
       " 'trip': 743,\n",
       " 'incredible': 744,\n",
       " 'translation': 745,\n",
       " 'then': 746,\n",
       " 'analysis': 747,\n",
       " 'hop': 748,\n",
       " 'prove': 749,\n",
       " 'reveal': 750,\n",
       " 'evidence': 751,\n",
       " 'quote': 752,\n",
       " 'western': 753,\n",
       " 'key': 754,\n",
       " 'belief': 755,\n",
       " 'understanding': 756,\n",
       " 'community': 757,\n",
       " 'genre': 758,\n",
       " 'hit': 759,\n",
       " 'introduce': 760,\n",
       " 'other': 761,\n",
       " 'train': 762,\n",
       " 'huge': 763,\n",
       " 'suspense': 764,\n",
       " 'childhood': 765,\n",
       " 'paper': 766,\n",
       " 'unfortunately': 767,\n",
       " 'german': 768,\n",
       " 'standard': 769,\n",
       " 'space': 770,\n",
       " 'familiar': 771,\n",
       " 'personality': 772,\n",
       " 'realistic': 773,\n",
       " 'comment': 774,\n",
       " 'cry': 775,\n",
       " 'encounter': 776,\n",
       " 'explanation': 777,\n",
       " 'himself': 778,\n",
       " 'remind': 779,\n",
       " 'robert': 780,\n",
       " 'imagination': 781,\n",
       " 'emotional': 782,\n",
       " 'except': 783,\n",
       " 'party': 784,\n",
       " 'career': 785,\n",
       " 'influence': 786,\n",
       " 'anne': 787,\n",
       " 'product': 788,\n",
       " 'discussion': 789,\n",
       " 'attack': 790,\n",
       " 'service': 791,\n",
       " 'pull': 792,\n",
       " 'actual': 793,\n",
       " 'generation': 794,\n",
       " 'explore': 795,\n",
       " 'resource': 796,\n",
       " 'hundred': 797,\n",
       " 'christ': 798,\n",
       " 'female': 799,\n",
       " 'compel': 800,\n",
       " 'red': 801,\n",
       " 'decision': 802,\n",
       " 'local': 803,\n",
       " 'none': 804,\n",
       " 'charm': 805,\n",
       " 'disappoint': 806,\n",
       " 'tool': 807,\n",
       " 'produce': 808,\n",
       " 'success': 809,\n",
       " 'era': 810,\n",
       " 'amount': 811,\n",
       " 'near': 812,\n",
       " 'pace': 813,\n",
       " 'popular': 814,\n",
       " 'york': 815,\n",
       " 'recently': 816,\n",
       " 'joy': 817,\n",
       " 'smith': 818,\n",
       " 'fully': 819,\n",
       " 'biography': 820,\n",
       " 'eat': 821,\n",
       " 'summer': 822,\n",
       " 'natural': 823,\n",
       " 'leader': 824,\n",
       " 'honest': 825,\n",
       " 'method': 826,\n",
       " 'mistake': 827,\n",
       " 'boring': 828,\n",
       " 'practical': 829,\n",
       " 'beautifully': 830,\n",
       " 'grade': 831,\n",
       " 'flaw': 832,\n",
       " 'trilogy': 833,\n",
       " 'technique': 834,\n",
       " 'obvious': 835,\n",
       " 'politics': 836,\n",
       " 'before': 837,\n",
       " 'paint': 838,\n",
       " 'lady': 839,\n",
       " 'ancient': 840,\n",
       " 'fire': 841,\n",
       " 'sea': 842,\n",
       " 'christmas': 843,\n",
       " 'informative': 844,\n",
       " 'jack': 845,\n",
       " 'manner': 846,\n",
       " 'heroine': 847,\n",
       " 'confuse': 848,\n",
       " 'cut': 849,\n",
       " 'fly': 850,\n",
       " 'win': 851,\n",
       " 'structure': 852,\n",
       " 'gain': 853,\n",
       " 'pleasure': 854,\n",
       " 'ten': 855,\n",
       " 'low': 856,\n",
       " 'french': 857,\n",
       " 'education': 858,\n",
       " 'freedom': 859,\n",
       " 'sell': 860,\n",
       " 'market': 861,\n",
       " 'throw': 862,\n",
       " 'water': 863,\n",
       " 'itself': 864,\n",
       " 'arrive': 865,\n",
       " 'deserve': 866,\n",
       " 'universe': 867,\n",
       " 'david': 868,\n",
       " 'feature': 869,\n",
       " 'henry': 870,\n",
       " 'express': 871,\n",
       " 'civil': 872,\n",
       " 'narrator': 873,\n",
       " 'obviously': 874,\n",
       " 'hat': 875,\n",
       " 'thoroughly': 876,\n",
       " 'strength': 877,\n",
       " 'outside': 878,\n",
       " 'tragedy': 879,\n",
       " 'alive': 880,\n",
       " 'artist': 881,\n",
       " 'technology': 882,\n",
       " 'recent': 883,\n",
       " 'rand': 884,\n",
       " 'skill': 885,\n",
       " 'appeal': 886,\n",
       " 'protagonist': 887,\n",
       " 'tom': 888,\n",
       " 'south': 889,\n",
       " 'tradition': 890,\n",
       " 'loss': 891,\n",
       " 'unlike': 892,\n",
       " 'fairly': 893,\n",
       " 'record': 894,\n",
       " 'principle': 895,\n",
       " 'thriller': 896,\n",
       " 'represent': 897,\n",
       " 'likely': 898,\n",
       " 'michael': 899,\n",
       " 'wild': 900,\n",
       " 'who': 901,\n",
       " 'essay': 902,\n",
       " 'longer': 903,\n",
       " 'passage': 904,\n",
       " 'toward': 905,\n",
       " 'position': 906,\n",
       " 'intelligent': 907,\n",
       " 'road': 908,\n",
       " 'genius': 909,\n",
       " 'error': 910,\n",
       " 'west': 911,\n",
       " 'impact': 912,\n",
       " 'project': 913,\n",
       " 'flow': 914,\n",
       " 'avoid': 915,\n",
       " 'professional': 916,\n",
       " 'destroy': 917,\n",
       " 'intend': 918,\n",
       " 'typical': 919,\n",
       " 'poetry': 920,\n",
       " 'necessary': 921,\n",
       " 'lay': 922,\n",
       " 'former': 923,\n",
       " 'trust': 924,\n",
       " 'model': 925,\n",
       " 'doctor': 926,\n",
       " 'indian': 927,\n",
       " 'essential': 928,\n",
       " 'personally': 929,\n",
       " 'epic': 930,\n",
       " 'sequel': 931,\n",
       " 'negative': 932,\n",
       " 'shock': 933,\n",
       " 'nation': 934,\n",
       " 'mary': 935,\n",
       " 'cold': 936,\n",
       " 'detailed': 937,\n",
       " 'horse': 938,\n",
       " 'positive': 939,\n",
       " 'map': 940,\n",
       " 'treasure': 941,\n",
       " 'army': 942,\n",
       " 'movement': 943,\n",
       " 'forever': 944,\n",
       " 'average': 945,\n",
       " 'vivid': 946,\n",
       " 'cook': 947,\n",
       " 'london': 948,\n",
       " 'basically': 949,\n",
       " 'usual': 950,\n",
       " 'portrait': 951,\n",
       " 'prejudice': 952,\n",
       " 'meaning': 953,\n",
       " 'professor': 954,\n",
       " 'apply': 955,\n",
       " 'scientific': 956,\n",
       " 'thanks': 957,\n",
       " 'catholic': 958,\n",
       " 'foundation': 959,\n",
       " 'successful': 960,\n",
       " 'through': 961,\n",
       " 'notice': 962,\n",
       " 'emma': 963,\n",
       " 'accurate': 964,\n",
       " 'behavior': 965,\n",
       " 'therefore': 966,\n",
       " 'soldier': 967,\n",
       " 'strike': 968,\n",
       " 'occur': 969,\n",
       " 'planet': 970,\n",
       " 'passion': 971,\n",
       " 'photo': 972,\n",
       " 'address': 973,\n",
       " 'promise': 974,\n",
       " 'terrible': 975,\n",
       " 'minute': 976,\n",
       " 'center': 977,\n",
       " 'brain': 978,\n",
       " 'lee': 979,\n",
       " 'christianity': 980,\n",
       " 'peter': 981,\n",
       " 'pride': 982,\n",
       " 'immediately': 983,\n",
       " 'ultimately': 984,\n",
       " 'paul': 985,\n",
       " 'wisdom': 986,\n",
       " 'format': 987,\n",
       " 'storyline': 988,\n",
       " 'stick': 989,\n",
       " 'deeply': 990,\n",
       " 'why': 991,\n",
       " 'stephen': 992,\n",
       " 'earlier': 993,\n",
       " 'impossible': 994,\n",
       " 'exercise': 995,\n",
       " 'portray': 996,\n",
       " 'satisfy': 997,\n",
       " 'path': 998,\n",
       " 'expectation': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b8e4a",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/word2vec-explained-49c52b4ccb71\n",
    "\n",
    "how to put the outputs of word2vec into data frame, and apply PCA on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29fbc5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '``' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/xfpwvv6x0fg3pkqm99zxr6z40000gq/T/ipykernel_68141/2640594824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m emb_df = (\n\u001b[1;32m      2\u001b[0m     pd.DataFrame(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;32m/var/folders/0s/xfpwvv6x0fg3pkqm99zxr6z40000gq/T/ipykernel_68141/2640594824.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m emb_df = (\n\u001b[1;32m      2\u001b[0m     pd.DataFrame(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2vmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '``' not present\""
     ]
    }
   ],
   "source": [
    "emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [w2v_model.wv.get_vector(str(n)) for n in w2vmodel.wv.key_to_index],\n",
    "        index = w2vmodel.wv.key_to_index\n",
    "    )\n",
    ")\n",
    "print(emb_df.shape)\n",
    "\n",
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model.wv.most_similar('wonderful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f264b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity between 'wonderful' \" +\n",
    "               \"terrible' - Skip Gram : \",\n",
    "    w2v_model.wv.similarity('wonderful', 'bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d67c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=7)\n",
    "pca_mdl = pca.fit_transform(emb_df)\n",
    "\n",
    "emb_df_PCA = (\n",
    "    pd.DataFrame(\n",
    "        pca_mdl,\n",
    "        columns=['x','y'],\n",
    "        index = emb_df.index\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.scatter(\n",
    "    x = emb_df_PCA['x'],\n",
    "    y = emb_df_PCA['y'],\n",
    "    s = 0.4,\n",
    "    color = 'maroon',\n",
    "    alpha = 0.5\n",
    ")\n",
    "\n",
    "plt.xlabel('PCA-1')\n",
    "plt.ylabel('PCA-2')\n",
    "plt.title('PCA Visualization')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0836d0",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/varun08/sentiment-analysis-using-word2vec/notebook\n",
    " define functions to calculate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5823884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    \n",
    "    if nwords ==0:\n",
    "        featureVec = np.zeros(1000)\n",
    "        #avoiding divide by zero, define it as a all zero vector\n",
    "    else:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # Dividing the result by number of words to get average\n",
    "\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Comment %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "            \n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(X_train['text'], w2v_model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e581c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unuseful_rows(trainDataVecs,y_train):\n",
    "    list=[]\n",
    "    for i in range(trainDataVecs.shape[0]):\n",
    "        if np.all(trainDataVecs[i,]==0):\n",
    "            list.append(i)\n",
    "    if len(list)!=0:\n",
    "        trainDataVecs=np.delete(trainDataVecs,list,axis=0)\n",
    "        y_train.drop(index=list,inplace=True)\n",
    "    return trainDataVecs,y_train\n",
    "# remove the sample with all zero vector and its corresponding y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff952d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1,y_train1=remove_unuseful_rows(trainDataVecs,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71810fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e056193",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed105b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "start_time = time.time()\n",
    "# Fit the model\n",
    "forest = forest.fit(trainDataVecs1, y_train1[\"score\"])\n",
    "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5f7b7",
   "metadata": {},
   "source": [
    "## Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size=1000\n",
    "window=3\n",
    "min_count=3\n",
    "workers=3\n",
    "sg=1\n",
    "\n",
    "start_t = time.time()\n",
    "test_w2vmodel=Word2Vec(X_test['text'],min_count=min_count,vector_size=vector_size,workers=workers,sg=sg)\n",
    "print('Time taken to test word2vec model:' +str(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='test_w2v_model.joblib'\n",
    "joblib.dump(test_w2vmodel,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2vmodel=joblib.load('test_w2v_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word2vec_model_file = '/Users/xin/Library/CloudStorage/OneDrive-UniversityofBristol/DST/DST_Assessment2/Xin/Data/' + 'test_word2vec_' + '.model'\n",
    "test_w2vmodel.save(test_word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25600045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v_model = Word2Vec.load(test_word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_df = (\n",
    "    pd.DataFrame(\n",
    "        [test_w2v_model.wv.get_vector(str(n)) for n in test_w2vmodel.wv.key_to_index],\n",
    "        index = test_w2vmodel.wv.key_to_index\n",
    "    )\n",
    ")\n",
    "print(test_emb_df.shape)\n",
    "\n",
    "test_emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs = getAvgFeatureVecs(X_test['text'], test_w2v_model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5123fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataVecs1,y_test1=remove_unuseful_rows(testDataVecs,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=forest.predict(testDataVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(prediction-y_test1['score'],prediction-y_test1['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction-y_test1['score']==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444844c",
   "metadata": {},
   "source": [
    "# Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train['text'])]\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "doc2vec_model = Doc2Vec(documents, vector_size=1000, window=3, min_count=1, workers=4)\n",
    "print('Time taken to train word2vec model:' +str(time.time()-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aff799",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='d2v_model.joblib'\n",
    "joblib.dump(doc2vec_model,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ae66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = doc2vec_model.infer_vector(X_train['text'][0])\n",
    "print(len(vector))\n",
    "print(\"Top 10 values in Doc2Vec inferred vector:\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_df = pd.read_csv(doc2vec_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_decision_doc2vec = DecisionTreeClassifier()\n",
    "# Fit the model\n",
    "clf_decision_doc2vec.fit(doc2vec_df, y_train['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14fc2c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nitin194/twitter-sentiment-analysis-word2vec-doc2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
