{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cf4ef5",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44a8194b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1579307",
   "metadata": {},
   "source": [
    "# load the data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6754422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('50k_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b850691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>good and helpfull read this book is very good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Sadly overpriced and irrelevant In spite of it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Endless rant Howard should have borrowed from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Not Quite Hip It's really a shame about the ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Journey to the Centre of the Earth Hey! This i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text\n",
       "0    3.0  good and helpfull read this book is very good ...\n",
       "1    1.0  Sadly overpriced and irrelevant In spite of it...\n",
       "2    2.0  Endless rant Howard should have borrowed from ...\n",
       "3    1.0  Not Quite Hip It's really a shame about the ti...\n",
       "4    5.0  Journey to the Centre of the Earth Hey! This i..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec7fbd",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333710e",
   "metadata": {},
   "source": [
    "In the above-given problem statement we have performed various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing emojis. The text document is then converted into the lowercase for better generalization.\n",
    "\n",
    "Subsequently, the punctuations were cleaned and removed thereby reducing the unnecessary noise from the dataset. After that, we have also removed the repeating characters from the words along with removing the URLs as they do not have any significant importance.\n",
    "\n",
    "At last, we then performed Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form known as lemma) for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a5fb1",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79d5a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "280e2626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9aca2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jl22174\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f852b3f",
   "metadata": {},
   "source": [
    "### Making statement text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "daa1af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it's really a shame about the ti...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']=df['text'].str.lower()\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10b9f8",
   "metadata": {},
   "source": [
    "### Restore common acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940dc3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a young boy in a\\ I am I isn not or blue coat and black snow pants , stading on a pail of skis in the snow.\n"
     ]
    }
   ],
   "source": [
    "s = \"a young boy in a\\ I'm I isn't or blue coat and black snow pants , stading on a pail of skis in the snow.\"\n",
    "#先去除特殊符号\n",
    "import re\n",
    "s = s.replace('/',' or ').replace('&',' and ')\n",
    "#设置缩略词的正则表达式\n",
    "pat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\",re.I)\n",
    "pat_s = re.compile(\"(?<=[a-zA-Z])\\'s\")\n",
    "pat_s2 = re.compile(\"(?<=s)\\'s?\")\n",
    "pat_not = re.compile(\"(?<=[a-zA-Z])\\'t\")\n",
    "pat_would = re.compile(\"(?<=[a-zA-Z])\\'d\")\n",
    "pat_will = re.compile(\"(?<=[a-zA-Z])\\'ll\")\n",
    "pat_am = re.compile(\"(?<=[I|i])\\'m\")\n",
    "pat_are = re.compile(\"(?<=[a-zA-Z])\\'re\")\n",
    "pat_ve = re.compile(\"(?<=[a-zA-Z])\\'ve\")\n",
    "#使用正则表达式进行展开\n",
    "new_text = pat_is.sub(r\"\\1 is\", s)\n",
    "new_text = pat_s.sub(\"\",new_text)\n",
    "new_text = pat_s2.sub(\"\",new_text)\n",
    "new_text = pat_not.sub(\" not\",new_text)\n",
    "new_text = pat_would.sub(\" would\",new_text)\n",
    "new_text = pat_will.sub(\" will\",new_text)\n",
    "new_text = pat_am.sub(\" am\",new_text)\n",
    "new_text = pat_are.sub(\" are\",new_text)\n",
    "new_text = pat_ve.sub(\" have\",new_text)\n",
    "new_text = new_text.replace('\\'',' ')\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9732db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_common_abbr(caption):\n",
    "    pat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n",
    "    pat_s = re.compile(\"(?<=[a-zA-Z])\\'s\")  # 找出字母后面的字母\n",
    "    pat_s2 = re.compile(\"(?<=s)\\'s?\")\n",
    "    pat_not = re.compile(\"(?<=[a-zA-Z])n\\'t\")  # not abbreviation\n",
    "    pat_would = re.compile(\"(?<=[a-zA-Z])\\'d\")  # would abbreviation\n",
    "    pat_will = re.compile(\"(?<=[a-zA-Z])\\'ll\")  # will abbreviation\n",
    "    pat_am = re.compile(\"(?<=[I|i])\\'m\")  # am abbreviation\n",
    "    pat_are = re.compile(\"(?<=[a-zA-Z])\\'re\")  # are abbreviation\n",
    "    pat_ve = re.compile(\"(?<=[a-zA-Z])\\'ve\")  # have abbreviation\n",
    "\n",
    "    new_text = caption\n",
    "    new_text = pat_is.sub(r\"\\1 is\", new_text)\n",
    "    new_text = pat_s.sub(\"\", new_text)\n",
    "    new_text = pat_s2.sub(\"\", new_text)\n",
    "    new_text = pat_not.sub(\" not\", new_text)\n",
    "    new_text = pat_would.sub(\" would\", new_text)\n",
    "    new_text = pat_will.sub(\" will\", new_text)\n",
    "    new_text = pat_am.sub(\" am\", new_text)\n",
    "    new_text = pat_are.sub(\" are\", new_text)\n",
    "    new_text = pat_ve.sub(\" have\", new_text)\n",
    "    new_text = new_text.replace('\\'', ' ')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "080368a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']= df['text'].apply(lambda x: restore_common_abbr(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcab67",
   "metadata": {},
   "source": [
    "### Cleaning and removing the above stop words list from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5b975c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have', 'my', 'so', 're', 'any', 'only', 'haven', 'between', 'be', 'am', \"hadn't\", 'its', 've', 'needn', 'you', 'doing', \"don't\", 'she', 'all', 'against', 'me', 'were', 'y', 'theirs', 'hadn', 'myself', 'should', \"didn't\", 'our', 'themselves', \"should've\", \"you'd\", 'where', 'a', 'hers', 'here', 'are', 'didn', 'below', 'is', 'this', 'ma', \"that'll\", \"mustn't\", 'weren', 'wouldn', 'their', 'ours', 'don', 'other', 'same', \"won't\", 'more', 'how', \"isn't\", 'few', 'than', 'too', 'in', 'if', 'yours', 'ain', 'yourself', \"wouldn't\", 'those', \"she's\", 'until', 'i', 'itself', 'has', 'shouldn', 'your', 'own', 'under', 'further', 'again', 'm', 'but', 'won', 'over', 'no', 'mustn', 'that', 'while', 'during', 'to', 'above', 'such', 'will', 'shan', \"wasn't\", 'him', 'them', 'was', 'he', 'yourselves', 'there', 'herself', 'for', \"doesn't\", \"couldn't\", 'doesn', 'not', 'did', 'hasn', 'with', 'some', 'into', 'does', 'nor', 'o', \"needn't\", 'and', 'most', 'his', 'having', 'after', \"you've\", 'the', 'they', 'then', 'isn', \"shan't\", 'once', 't', 'it', \"shouldn't\", 'been', 'can', 'being', 'of', 'about', 'just', 'down', \"hasn't\", 'now', 'ourselves', 'd', 'up', 'her', 'had', 'aren', 'what', 'from', \"you'll\", 'each', 'we', 'whom', \"mightn't\", 'do', 'because', 'through', 's', \"weren't\", 'himself', 'who', 'both', 'an', 'by', 'when', 'at', 'as', 'off', 'very', 'these', 'or', \"it's\", 'mightn', 'before', \"you're\", \"haven't\", 'on', 'why', 'll', 'which', 'wasn', \"aren't\", 'couldn', 'out'}\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cf1f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our new stopoing words to keep some words like 'not'  \n",
    "my_stopwords =[{'have', 'my', 'so', 're', 'any', 'only', 'haven', 'between', 'be', 'am', \"hadn't\", 'its', 've', 'needn', 'you', 'doing', \"don't\", 'she', 'all', 'against', 'me', 'were', 'y', 'theirs', 'hadn', 'myself', 'should', \"didn't\", 'our', 'themselves', \"should've\", \"you'd\", 'where', 'a', 'hers', 'here', 'are', 'didn', 'below', 'is', 'this', 'ma', \"that'll\", \"mustn't\", 'weren', 'wouldn', 'their', 'ours', 'don', 'other', 'same', \"won't\", 'more', 'how', \"isn't\", 'few', 'than', 'too', 'in', 'if', 'yours', 'ain', 'yourself', \"wouldn't\", 'those', \"she's\", 'until', 'i', 'itself', 'has', 'shouldn', 'your', 'own', 'under', 'further', 'again', 'm', 'but', 'won', 'over', 'no', 'mustn', 'that', 'while', 'during', 'to', 'above', 'such', 'will', 'shan', \"wasn't\", 'him', 'them', 'was', 'he', 'yourselves', 'there', 'herself', 'for', \"doesn't\", \"couldn't\", 'doesn', 'did', 'hasn', 'with', 'some', 'into', 'does', 'nor', 'o', \"needn't\", 'and', 'most', 'his', 'having', 'after', \"you've\", 'the', 'they', 'then', 'isn', \"shan't\", 'once', 't', 'it', \"shouldn't\", 'been', 'can', 'being', 'of', 'about', 'just', 'down', \"hasn't\", 'now', 'ourselves', 'd', 'up', 'her', 'had', 'aren', 'what', 'from', \"you'll\", 'each', 'we', 'whom', \"mightn't\", 'do', 'because', 'through', 's', \"weren't\", 'himself', 'who', 'both', 'an', 'by', 'when', 'at', 'as', 'off', 'very', 'these', 'or', \"it's\", 'mightn', 'before', \"you're\", \"haven't\", 'on', 'why', 'll', 'which', 'wasn', \"aren't\", 'couldn', 'out'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab13476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in my_stopwords])\n",
    "df['text'] = df['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bfdcc",
   "metadata": {},
   "source": [
    "### Cleaning and removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f72201a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['text']= df['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ef081",
   "metadata": {},
   "source": [
    "### Cleaning and removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c692e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fea2b4",
   "metadata": {},
   "source": [
    "### Cleaning and removing Numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40ac519d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', ' ', data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_numbers(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146d3f7",
   "metadata": {},
   "source": [
    "### Remove short words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c668587",
   "metadata": {},
   "source": [
    "We remove those words which are of little or no use. So, we will select the length of words which we want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "639b3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book very good you...\n",
       "1    sadly overpriced and irrelevant spite its clai...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip really shame about the time and ...\n",
       "4    journey the centre the earth hey this great bo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_text(text):\n",
    "    return ' '.join([word for word in text.split() if len(word) > 2])\n",
    "df['text'] = df['text'].apply(lambda x: transform_text(x))\n",
    "df['text'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b14fd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e884061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "df['text']=df['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff6bab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8579f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jl22174\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83b8e5",
   "metadata": {},
   "source": [
    "### Applying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23cba065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df['text']= df['text'].apply(lambda x: stemming_on_text(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76b0e2",
   "metadata": {},
   "source": [
    "### Applying Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbbe90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jl22174\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfd8db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jl22174\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd76b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df['text'] = df['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec93347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jl22174\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43749196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that', 'involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to', 'score', 'a', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentence = 'football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.'\n",
    "tokens = word_tokenize(sentence)  # 分词\n",
    "tagged_sent = pos_tag(tokens)     # 获取单词词性\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas_sent = []\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "\n",
    "print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03962b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m tagged_sent \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m wnl \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m     20\u001b[0m lemmas_sent \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    177\u001b[0m prev, prev2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART\n\u001b[0;32m    178\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 180\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEND\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m    182\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    183\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdict\u001b[38;5;241m.\u001b[39mget(word), \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_tagdict \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    177\u001b[0m prev, prev2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART\n\u001b[0;32m    178\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 180\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEND\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m    182\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    183\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdict\u001b[38;5;241m.\u001b[39mget(word), \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_tagdict \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[0m, in \u001b[0;36mPerceptronTagger.normalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;129;01mand\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!HYPHEN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdigit\u001b[49m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!YEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mand\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misdigit():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "tagged_sent = pos_tag(df['text'])\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas_sent = []\n",
    "\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be568eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
