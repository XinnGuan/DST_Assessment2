{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cf4ef5",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a8194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1579307",
   "metadata": {},
   "source": [
    "# load the data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6754422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('50k_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b850691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>good and helpfull read this book is very good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Sadly overpriced and irrelevant In spite of it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Endless rant Howard should have borrowed from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Not Quite Hip It's really a shame about the ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Journey to the Centre of the Earth Hey! This i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text\n",
       "0    3.0  good and helpfull read this book is very good ...\n",
       "1    1.0  Sadly overpriced and irrelevant In spite of it...\n",
       "2    2.0  Endless rant Howard should have borrowed from ...\n",
       "3    1.0  Not Quite Hip It's really a shame about the ti...\n",
       "4    5.0  Journey to the Centre of the Earth Hey! This i..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec7fbd",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a333710e",
   "metadata": {},
   "source": [
    "In the above-given problem statement we have performed various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing emojis. The text document is then converted into the lowercase for better generalization.\n",
    "\n",
    "Subsequently, the punctuations were cleaned and removed thereby reducing the unnecessary noise from the dataset. After that, we have also removed the repeating characters from the words.\n",
    "\n",
    "At last, we then performed Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form known as lemma) with limitation of part of speech for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a5fb1",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79d5a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280e2626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aca2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f852b3f",
   "metadata": {},
   "source": [
    "### Making statement text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daa1af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it's really a shame about the ti...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']=df['text'].str.lower()\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10b9f8",
   "metadata": {},
   "source": [
    "### Restore common acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9732db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_common_abbr(caption):\n",
    "    pat_is = re.compile(\"(it|he|she|that|this|there|here)(\\'s)\", re.I)\n",
    "    pat_s = re.compile(\"(?<=[a-zA-Z])\\'s\")  \n",
    "    pat_s2 = re.compile(\"(?<=s)\\'s?\")\n",
    "    pat_not = re.compile(\"(?<=[a-zA-Z])n\\'t\")  # not abbreviation\n",
    "    pat_would = re.compile(\"(?<=[a-zA-Z])\\'d\")  # would abbreviation\n",
    "    pat_will = re.compile(\"(?<=[a-zA-Z])\\'ll\")  # will abbreviation\n",
    "    pat_am = re.compile(\"(?<=[I|i])\\'m\")  # am abbreviation\n",
    "    pat_are = re.compile(\"(?<=[a-zA-Z])\\'re\")  # are abbreviation\n",
    "    pat_ve = re.compile(\"(?<=[a-zA-Z])\\'ve\")  # have abbreviation\n",
    "\n",
    "    new_text = caption\n",
    "    new_text = pat_is.sub(r\"\\1 is\", new_text)\n",
    "    new_text = pat_s.sub(\"\", new_text)\n",
    "    new_text = pat_s2.sub(\"\", new_text)\n",
    "    new_text = pat_not.sub(\" not\", new_text)\n",
    "    new_text = pat_would.sub(\" would\", new_text)\n",
    "    new_text = pat_will.sub(\" will\", new_text)\n",
    "    new_text = pat_am.sub(\" am\", new_text)\n",
    "    new_text = pat_are.sub(\" are\", new_text)\n",
    "    new_text = pat_ve.sub(\" have\", new_text)\n",
    "    new_text = new_text.replace('\\'', ' ')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "080368a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']= df['text'].apply(lambda x: restore_common_abbr(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcab67",
   "metadata": {},
   "source": [
    "### Cleaning and removing the above stop words list from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5b975c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have', 'my', 'so', 're', 'any', 'only', 'haven', 'between', 'be', 'am', \"hadn't\", 'its', 've', 'needn', 'you', 'doing', \"don't\", 'she', 'all', 'against', 'me', 'were', 'y', 'theirs', 'hadn', 'myself', 'should', \"didn't\", 'our', 'themselves', \"should've\", \"you'd\", 'where', 'a', 'hers', 'here', 'are', 'didn', 'below', 'is', 'this', 'ma', \"that'll\", \"mustn't\", 'weren', 'wouldn', 'their', 'ours', 'don', 'other', 'same', \"won't\", 'more', 'how', \"isn't\", 'few', 'than', 'too', 'in', 'if', 'yours', 'ain', 'yourself', \"wouldn't\", 'those', \"she's\", 'until', 'i', 'itself', 'has', 'shouldn', 'your', 'own', 'under', 'further', 'again', 'm', 'but', 'won', 'over', 'no', 'mustn', 'that', 'while', 'during', 'to', 'above', 'such', 'will', 'shan', \"wasn't\", 'him', 'them', 'was', 'he', 'yourselves', 'there', 'herself', 'for', \"doesn't\", \"couldn't\", 'doesn', 'not', 'did', 'hasn', 'with', 'some', 'into', 'does', 'nor', 'o', \"needn't\", 'and', 'most', 'his', 'having', 'after', \"you've\", 'the', 'they', 'then', 'isn', \"shan't\", 'once', 't', 'it', \"shouldn't\", 'been', 'can', 'being', 'of', 'about', 'just', 'down', \"hasn't\", 'now', 'ourselves', 'd', 'up', 'her', 'had', 'aren', 'what', 'from', \"you'll\", 'each', 'we', 'whom', \"mightn't\", 'do', 'because', 'through', 's', \"weren't\", 'himself', 'who', 'both', 'an', 'by', 'when', 'at', 'as', 'off', 'very', 'these', 'or', \"it's\", 'mightn', 'before', \"you're\", \"haven't\", 'on', 'why', 'll', 'which', 'wasn', \"aren't\", 'couldn', 'out'}\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cf1f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our new stopoing words to keep some words like 'not'  \n",
    "my_stopwords =[{'have', 'my', 'so', 're', 'any', 'only', 'haven', 'between', 'be', 'am', \"hadn't\", 'its', 've', 'needn', 'you', 'doing', \"don't\", 'she', 'all', 'against', 'me', 'were', 'y', 'theirs', 'hadn', 'myself', 'should', \"didn't\", 'our', 'themselves', \"should've\", \"you'd\", 'where', 'a', 'hers', 'here', 'are', 'didn', 'below', 'is', 'this', 'ma', \"that'll\", \"mustn't\", 'weren', 'wouldn', 'their', 'ours', 'don', 'other', 'same', \"won't\", 'more', 'how', \"isn't\", 'few', 'than', 'too', 'in', 'if', 'yours', 'ain', 'yourself', \"wouldn't\", 'those', \"she's\", 'until', 'i', 'itself', 'has', 'shouldn', 'your', 'own', 'under', 'further', 'again', 'm', 'but', 'won', 'over', 'no', 'mustn', 'that', 'while', 'during', 'to', 'above', 'such', 'will', 'shan', \"wasn't\", 'him', 'them', 'was', 'he', 'yourselves', 'there', 'herself', 'for', \"doesn't\", \"couldn't\", 'doesn', 'did', 'hasn', 'with', 'some', 'into', 'does', 'nor', 'o', \"needn't\", 'and', 'most', 'his', 'having', 'after', \"you've\", 'the', 'they', 'then', 'isn', \"shan't\", 'once', 't', 'it', \"shouldn't\", 'been', 'can', 'being', 'of', 'about', 'just', 'down', \"hasn't\", 'now', 'ourselves', 'd', 'up', 'her', 'had', 'aren', 'what', 'from', \"you'll\", 'each', 'we', 'whom', \"mightn't\", 'do', 'because', 'through', 's', \"weren't\", 'himself', 'who', 'both', 'an', 'by', 'when', 'at', 'as', 'off', 'very', 'these', 'or', \"it's\", 'mightn', 'before', \"you're\", \"haven't\", 'on', 'why', 'll', 'which', 'wasn', \"aren't\", 'couldn', 'out'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab13476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey! this i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in my_stopwords])\n",
    "df['text'] = df['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bfdcc",
   "metadata": {},
   "source": [
    "### Cleaning and removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f72201a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['text']= df['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ef081",
   "metadata": {},
   "source": [
    "### Cleaning and removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c692e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fea2b4",
   "metadata": {},
   "source": [
    "### Cleaning and removing Numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40ac519d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book is very good ...\n",
       "1    sadly overpriced and irrelevant in spite of it...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip it is really a shame about the t...\n",
       "4    journey to the centre of the earth hey this is...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', ' ', data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_numbers(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146d3f7",
   "metadata": {},
   "source": [
    "### Remove short words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c668587",
   "metadata": {},
   "source": [
    "We remove those words which are of little or no use. So, we will select the length of words which we want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "639b3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    good and helpfull read this book very good you...\n",
       "1    sadly overpriced and irrelevant spite its clai...\n",
       "2    endless rant howard should have borrowed from ...\n",
       "3    not quite hip really shame about the time and ...\n",
       "4    journey the centre the earth hey this great bo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_text(text):\n",
    "    return ' '.join([word for word in text.split() if len(word) > 2])\n",
    "df['text'] = df['text'].apply(lambda x: transform_text(x))\n",
    "df['text'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b14fd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931498ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\haile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e884061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "df['text']=df['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff6bab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83b8e5",
   "metadata": {},
   "source": [
    "### Applying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23cba065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df['text']= df['text'].apply(lambda x: stemming_on_text(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76b0e2",
   "metadata": {},
   "source": [
    "### Applying Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbbe90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haile\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfd8db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\haile\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd76b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overpriced, and, irrelevant, spite, it...\n",
       "2    [endless, rant, howard, should, have, borrowed...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df['text'] = df['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec93347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\haile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43749196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that', 'involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to', 'score', 'a', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentence = 'football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.'\n",
    "tokens = word_tokenize(sentence)  # 分词\n",
    "tagged_sent = pos_tag(tokens)     # 获取单词词性\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas_sent = []\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # 词形还原\n",
    "\n",
    "print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03962b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [good, and, helpfull, read, this, book, very, ...\n",
       "1    [sadly, overprice, and, irrelevant, spite, it,...\n",
       "2    [endless, rant, howard, should, have, borrow, ...\n",
       "3    [not, quite, hip, really, shame, about, the, t...\n",
       "4    [journey, the, centre, the, earth, hey, this, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# get the part of speech\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def tag_lem(doc,lemmatizer):\n",
    "    if doc[-1] != '.':\n",
    "        doc.append('.')\n",
    "    tagged_sent = pos_tag(doc)  # get the part of speech\n",
    "\n",
    "    new_s = []\n",
    "    for c in tagged_sent:\n",
    "        if c[0].isdigit():\n",
    "            new_s.append(\"#number\")\n",
    "        elif c[0] not in string.punctuation:\n",
    "            wordnet_pos = get_wordnet_pos(c[1]) or wordnet.NOUN\n",
    "            new_s.append(lemmatizer.lemmatize(c[0], pos=wordnet_pos))  \n",
    "    return new_s\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: tag_lem(x,lemmatizer))\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4be568eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_cleaned.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e233b3edd5e60f0904e3f8404c7ce821d9d17539f44ffe7eab4eba0129924986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
