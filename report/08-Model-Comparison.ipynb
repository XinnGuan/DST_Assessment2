{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e067184",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cefb6",
   "metadata": {},
   "source": [
    "Now that we have attained the models trained on the training set, we must compare the performance of these models on the test set. First we import some necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efff4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588b9b3",
   "metadata": {},
   "source": [
    "# 1. Retrieve Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f464c",
   "metadata": {},
   "source": [
    "Each model has a list of predictions for the test set where they give their respective probabilities of the review being positive. We read each list of probabilities in and store them in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bf6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda200k_combined_df = pd.read_csv('pred_probs_combined_50k150.csv')\n",
    "lda200k_summaries_df = pd.read_csv('pred_probs_summaries_50k150.csv')\n",
    "\n",
    "w2v200k_combined_df = pd.read_csv('combined_proba_results.csv')\n",
    "#w2v200k_summaries_df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee75b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda200k_combined = lda200k_combined_df['Probability of positive']\n",
    "lda200k_summaries = lda200k_summaries_df['Probability of positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d7e67",
   "metadata": {},
   "source": [
    "# 2. Retrieve test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20398988",
   "metadata": {},
   "source": [
    "As topic modelling is an unsupervised process, we have to load the test dataset with the sentiment scores (score of 0 is a negative review, score of 1 is a positive review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386f1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_sentiments_50k.csv')\n",
    "test = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db716363",
   "metadata": {},
   "source": [
    "# 3. Producing curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae4fc9",
   "metadata": {},
   "source": [
    "We now take the probabilities produced by the models and produce the ROC based on these values and the true values stored in \"test\".\n",
    "\n",
    "Here, we generate the points for the ROC curves using the \"roc_curve\" function in the sk-learn \"metrics\" package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d9aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "lda_200k_combined_fpr, lda_200k_combined_tpr, _ = roc_curve(test, lda200k_combined)\n",
    "lda_200k_summaries_fpr, lda_200k_summaries_tpr, _ = roc_curve(test, lda200k_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0de992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
