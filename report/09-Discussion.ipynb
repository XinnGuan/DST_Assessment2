{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b8d0e5",
   "metadata": {},
   "source": [
    "# Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdd1c4",
   "metadata": {},
   "source": [
    "## 1. Performance Metrics for Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d6060",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of our dataset, we decided against using accuracy as a performance metric. This is because we could achieve a really high accuracy score by simply predicting that all observations belong to the majority class (positive reviews). However, seeing as we cared equally about our topic models' ability to detect positive and negative sentiment, this seemed like a bad idea.\n",
    "\n",
    "Initially, we investigate the ROC curve which shows you sensitivity and specificity at all possible thresholds. So if you find a point that represents the right tradeoff, you can choose the threshold that goes with that point on the curve. However, some literature argues that models trained on imbalanced datasets may seem to perform well when you look at an ROC curve, but when looking at the precision recall curve they do not perform well at all [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/). \n",
    "\n",
    "When evaluating just by the ROC curve, ................... was almost a winner at all thresholds as most of the curve lies above all the other model curves. It therefore also had the highest AUC score. [The Relationship Between Precision-Recall and ROC Curves, 2006](https://www.biostat.wisc.edu/~page/rocpr.pdf) states that if a curve dominates in ROC, it also dominates in PR, which was also the case when we plotted it. Due to the dimensionality of the features (Word2Vec had 100 whereas LDA had 58), we expected the Word2Vec model to outperform LDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690ea6e",
   "metadata": {},
   "source": [
    "## 2. LDA Model (N-grams vs no n-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69a688",
   "metadata": {},
   "source": [
    "In the LDA model development, two models and corresponding sets of predictions were produced. The aim of this was to see if the adapting the text to concatenate common bigrams would improve the topic model's performance. In the performance evaluation, it was decided that model which did not include bigrams performed better. However, due to time constraints of the project, hyperparameter tuning was only able to take place for the no bigram model, and therefore it is not possible to put the results down to one or the other of hyperparameter tuning or retention of bigrams.\n",
    "\n",
    "We can conclude that at least one of these approaches improved the model and a better result would be achieved out of it if the predictions were to be used in the real world, but more investigation would be required to know the true causation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc560c",
   "metadata": {},
   "source": [
    "## 3. Review Summaries vs Review Summaries & Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938218e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "908e92c8",
   "metadata": {},
   "source": [
    "## 4. Limitations of Our Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35481d9",
   "metadata": {},
   "source": [
    "We recognise that our project is not fully polished and there are limitations to the conclusions we have drawn.\n",
    "* Our models could not train on the full dataset hence we cannot predict how well they will generalise to the full population.\n",
    "* For the logistic regression model, there was a trade off between information loss and dimensionality reduction when implementing PCA.\n",
    "* The main limitation of logistic regression is the assumption of linearity between the dependent and independent variables.\n",
    "* The selected KNN model used an approximate search algorithm (KD Tree) for time saving purposes, however as the Mahalanobis distance metric is incompatible with KD Trees, the Euclidean distance was used. This is a limitation as the Euclidean distance cannot detect high correlation between variables.\n",
    "* In the BDT models, the class imbalance weighting correction used was incorrect and due to the time restraints was not able to be rectified. This means that the results are probably worse than they could have been.\n",
    "* We allowed models to train on different datasets (dataset with missing values and the imputed dataset). This did not allow us to make fair comparisons.\n",
    "* Our imputation method only achieved a root mean squared error of 0.33, which considering the standardised data took values between 0 and 1, this is not very good. However, some columns had less than 5% missingness so the effect of the imputation may be negligble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afd0cc",
   "metadata": {},
   "source": [
    "## 5. Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7b805",
   "metadata": {},
   "source": [
    "Given the time constraints and difficulties we encountered, there are numerous avenues for further exploration with our data. Some of them are listed below:\n",
    "* Using Bluecrystal HPC would have massively reduced computing time and would have allowed us to train our models on a larger dataset. We hypothesise that our models would perform better on larger datasets due to the nature of how topic modelling works.\n",
    "* Implementing cross validation when using grid search for the LDA model. This may have allowed us to choose a more suitable number of topics.\n",
    "* Investigate different hyperparameter tuning methods such as Bayesian optimisation.\n",
    "* Investigate other negation detection methods to improve our sentiment analysis for LDA.\n",
    "* To further improve our classification model, we could incorportate other features of the data, such as the review helpfulness score, which could act as a weighting for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb429d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
